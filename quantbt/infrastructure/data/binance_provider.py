"""
ë°”ì´ë‚¸ìŠ¤ ë°ì´í„° í”„ë¡œë°”ì´ë” - SQLite + Parquet ê¸°ë°˜

ë°”ì´ë‚¸ìŠ¤ APIë¥¼ í™œìš©í•œ ì•”í˜¸í™”í ë°ì´í„° ì œê³µì
"""

import asyncio
import aiohttp
import csv
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, timezone, timedelta
import polars as pl
import json

from ...core.interfaces.data_provider import DataProviderBase
from ...core.utils.timeframe import TimeframeUtils
from ...config import DB_PATH, CACHE_DIR
from .database_manager import DatabaseManager
from .cache_manager import CacheManager


class BinanceDataProvider(DataProviderBase):
    """ë°”ì´ë‚¸ìŠ¤ API ê¸°ë°˜ ë°ì´í„° ì œê³µì - SQLite + Parquet"""
    
    SUPPORTED_API_TIMEFRAMES = [
        "1m", "3m", "5m", "15m", "30m", "1h", "2h", "4h", "6h", "8h", "12h", "1d", "3d", "1w", "1M"
    ]
    PROVIDER_NAME = "binance"
    
    def __init__(
        self,
        db_path: str = None,
        cache_dir: str = None,
        rate_limit_delay: float = 0.05,  # 1200 req/min = 20 req/sec
        max_candles_per_request: int = 1000
    ):
        super().__init__("BinanceDataProvider")
        self.base_url = "https://api.binance.com/api/v3"
        self.rate_limit_delay = rate_limit_delay
        self.max_candles_per_request = max_candles_per_request
        
        # config.pyì—ì„œ ì ˆëŒ€ê²½ë¡œ ì‚¬ìš©
        if db_path is None:
            db_path = DB_PATH
        if cache_dir is None:
            cache_dir = CACHE_DIR
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë° ìºì‹œ ë§¤ë‹ˆì €
        self.db_manager = DatabaseManager(db_path)
        self.cache_manager = CacheManager(cache_dir, db_path)
        
        # HTTP ì„¸ì…˜
        self._session: Optional[aiohttp.ClientSession] = None
        
        # ì‹¬ë³¼ ìºì‹œ
        self._all_symbols_cache: Optional[List[str]] = None
        self._usdt_symbols_cache: Optional[List[str]] = None
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """HTTP ì„¸ì…˜ ë°˜í™˜ - íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ì„¤ì •"""
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(
                    total=10,  # ì „ì²´ ìš”ì²­ 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                    connect=3,  # ì—°ê²° 3ì´ˆ íƒ€ì„ì•„ì›ƒ
                    sock_read=5  # ì†Œì¼“ ì½ê¸° 5ì´ˆ íƒ€ì„ì•„ì›ƒ
                ),
                headers={
                    "accept": "application/json",
                    "user-agent": "QuantBT/1.0"
                },
                connector=aiohttp.TCPConnector(
                    limit=20,  # ë™ì‹œ ì—°ê²° ì œí•œ
                    ttl_dns_cache=300,  # DNS ìºì‹œ 5ë¶„
                    use_dns_cache=True
                )
            )
        return self._session
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self._session and not self._session.closed:
            await self._session.close()
            self._session = None
    
    def _normalize_timezone(self, dt: datetime) -> datetime:
        """datetimeì„ UTCë¡œ ì •ê·œí™”"""
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        else:
            return dt.astimezone(timezone.utc)
    
    def _to_utc(self, dt: datetime) -> datetime:
        """datetimeì„ UTCë¡œ ë³€í™˜"""
        return self._normalize_timezone(dt)
    
    def _parse_binance_kline(self, kline_data: list, symbol: str) -> dict:
        """ë°”ì´ë‚¸ìŠ¤ kline ë°ì´í„°ë¥¼ í‘œì¤€ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        return {
            "timestamp": datetime.fromtimestamp(kline_data[0] / 1000, tz=timezone.utc),
            "symbol": symbol,
            "open": float(kline_data[1]),
            "high": float(kline_data[2]),
            "low": float(kline_data[3]),
            "close": float(kline_data[4]),
            "volume": float(kline_data[5])
        }
    
    def _fill_missing_data(self, df: pl.DataFrame) -> pl.DataFrame:
        """ë¹ˆê°’ì„ ì´ì „ê°’ìœ¼ë¡œ ì±„ìš°ê¸°"""
        if df.is_empty():
            return df
        
        # ì‹¬ë³¼ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì²˜ë¦¬
        result_dfs = []
        
        for symbol in df["symbol"].unique():
            symbol_df = df.filter(pl.col("symbol") == symbol).sort("timestamp")
            
            # forward fill ìˆ˜í–‰
            filled_df = symbol_df.with_columns([
                pl.col("open").fill_null(strategy="forward"),
                pl.col("high").fill_null(strategy="forward"),
                pl.col("low").fill_null(strategy="forward"),
                pl.col("close").fill_null(strategy="forward"),
                pl.col("volume").fill_null(strategy="forward")
            ])
            
            result_dfs.append(filled_df)
        
        if result_dfs:
            return pl.concat(result_dfs).sort(["symbol", "timestamp"])
        else:
            return df
    
    async def get_all_symbols(self) -> List[str]:
        """ì „ì²´ ì‹¬ë³¼ ëª©ë¡ ì¡°íšŒ"""
        if self._all_symbols_cache is not None:
            return self._all_symbols_cache
        
        session = await self._get_session()
        
        try:
            async with session.get(f"{self.base_url}/ticker/24hr") as response:
                response.raise_for_status()
                tickers = await response.json()
                
                symbols = [ticker["symbol"] for ticker in tickers]
                self._all_symbols_cache = symbols
                return symbols
                
        except Exception as e:
            print(f"âš ï¸ ì „ì²´ ì‹¬ë³¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def get_usdt_symbols(self) -> List[str]:
        """USDT í˜ì–´ ì‹¬ë³¼ë§Œ ì¡°íšŒ"""
        if self._usdt_symbols_cache is not None:
            return self._usdt_symbols_cache
        
        all_symbols = await self.get_all_symbols()
        usdt_symbols = [symbol for symbol in all_symbols if symbol.endswith("USDT")]
        self._usdt_symbols_cache = usdt_symbols
        return usdt_symbols
    
    def _load_available_symbols(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¬ë³¼ ë¡œë“œ - ì£¼ìš” USDT í˜ì–´"""
        return [
            "BTCUSDT", "ETHUSDT", "BNBUSDT", "XRPUSDT", "ADAUSDT",
            "SOLUSDT", "DOTUSDT", "DOGEUSDT", "AVAXUSDT", "LINKUSDT",
            "MATICUSDT", "LTCUSDT", "UNIUSDT", "ATOMUSDT", "FILUSDT",
            "MANAUSDT", "SANDUSDT", "AXSUSDT", "ICPUSDT", "NEARUSDT"
        ]
    
    def validate_timeframe(self, timeframe: str) -> bool:
        """ëª¨ë“  íƒ€ì„í”„ë ˆì„ ì§€ì› (ë¦¬ìƒ˜í”Œë§ ì²˜ë¦¬)"""
        return TimeframeUtils.validate_timeframe(timeframe)
    
    def validate_date_range(self, start: datetime, end: datetime) -> bool:
        """ë‚ ì§œ ë²”ìœ„ ìœ íš¨ì„± ê²€ì¦"""
        start_normalized = self._normalize_timezone(start)
        end_normalized = self._normalize_timezone(end)
        now_utc = datetime.now(timezone.utc)
        
        return start_normalized <= end_normalized and start_normalized <= now_utc
    
    async def _check_network_connection(self) -> bool:
        """ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            session = await self._get_session()
            async with session.get(f"{self.base_url}/ping") as response:
                return response.status == 200
        except Exception:
            return False
    
    def _get_existing_data_ranges(
        self, 
        symbol: str, 
        timeframe: str, 
        start_utc: datetime, 
        end_utc: datetime
    ) -> List[Dict[str, datetime]]:
        """DBì—ì„œ ê¸°ì¡´ ë°ì´í„°ì˜ ì‹¤ì œ ë²”ìœ„ë“¤ì„ ì¡°íšŒ"""
        try:
            import sqlite3
            ranges = []
            
            with sqlite3.connect(self.db_manager.db_path) as conn:
                cursor = conn.execute('''
                    SELECT timestamp_utc 
                    FROM market_data 
                    WHERE provider = ? AND symbol = ? AND timeframe = ?
                    AND timestamp_utc >= ? AND timestamp_utc <= ?
                    ORDER BY timestamp_utc
                ''', (
                    self.PROVIDER_NAME, 
                    symbol, 
                    timeframe,
                    start_utc.isoformat(),
                    end_utc.isoformat()
                ))
                
                timestamps = []
                for row in cursor.fetchall():
                    timestamp_str = row[0]
                    if timestamp_str.endswith("Z"):
                        timestamp_str = timestamp_str.replace("Z", "+00:00")
                    timestamp = datetime.fromisoformat(timestamp_str)
                    if timestamp.tzinfo is None:
                        timestamp = timestamp.replace(tzinfo=timezone.utc)
                    timestamps.append(timestamp)
                
                if not timestamps:
                    return ranges
                
                # ì—°ì†ëœ ë°ì´í„° êµ¬ê°„ì„ ì°¾ì•„ì„œ ë²”ìœ„ë¡œ ê·¸ë£¹í•‘
                if timeframe == "1m":
                    gap_threshold = timedelta(minutes=10)
                elif timeframe == "1d":
                    gap_threshold = timedelta(days=2)
                else:
                    gap_threshold = timedelta(hours=2)
                
                current_start = timestamps[0]
                current_end = timestamps[0]
                
                for i in range(1, len(timestamps)):
                    current_ts = timestamps[i]
                    
                    if current_ts - current_end <= gap_threshold:
                        current_end = current_ts
                    else:
                        ranges.append({
                            "start": current_start,
                            "end": current_end
                        })
                        current_start = current_ts
                        current_end = current_ts
                
                ranges.append({
                    "start": current_start,
                    "end": current_end
                })
                
            return ranges
            
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ë²”ìœ„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def _calculate_missing_periods(
        self,
        symbol: str,
        timeframe: str,
        requested_start: datetime,
        requested_end: datetime,
        existing_ranges: List[Dict[str, datetime]]
    ) -> List[Dict[str, datetime]]:
        """ìš”ì²­ ê¸°ê°„ì—ì„œ ê¸°ì¡´ ë°ì´í„° ë²”ìœ„ë¥¼ ì œì™¸í•œ ë¹ˆ ê¸°ê°„ë“¤ì„ ê³„ì‚°"""
        if not existing_ranges:
            return [{
                "start": requested_start,
                "end": requested_end
            }]
        
        missing_periods = []
        
        sorted_ranges = sorted(existing_ranges, key=lambda x: x["start"])
        
        if requested_start.tzinfo is None:
            requested_start = requested_start.replace(tzinfo=timezone.utc)
        if requested_end.tzinfo is None:
            requested_end = requested_end.replace(tzinfo=timezone.utc)
        
        current_time = requested_start
        
        for existing_range in sorted_ranges:
            range_start = existing_range["start"]
            range_end = existing_range["end"]
            
            if range_start.tzinfo is None:
                range_start = range_start.replace(tzinfo=timezone.utc)
            if range_end.tzinfo is None:
                range_end = range_end.replace(tzinfo=timezone.utc)
            
            if current_time < range_start:
                missing_periods.append({
                    "start": current_time,
                    "end": range_start
                })
            
            current_time = max(current_time, range_end)
        
        if current_time < requested_end:
            missing_periods.append({
                "start": current_time,
                "end": requested_end
            })
        
        return missing_periods
    
    async def _fetch_candles_from_api(
        self,
        symbol: str,
        timeframe: str,
        start: datetime,
        end: datetime
    ) -> pl.DataFrame:
        """ë°”ì´ë‚¸ìŠ¤ APIì—ì„œ ìº”ë“¤ ë°ì´í„° ì¡°íšŒ"""
        if timeframe not in self.SUPPORTED_API_TIMEFRAMES:
            raise ValueError(f"API timeframe not supported: {timeframe}")
        
        # UTCë¡œ ì •ê·œí™”
        start_utc = self._normalize_timezone(start)
        end_utc = self._normalize_timezone(end)
        
        session = await self._get_session()
        all_candles = []
        
        url = f"{self.base_url}/klines"
        current_start = start_utc
        
        max_requests = 10000
        request_count = 0
        start_time = asyncio.get_event_loop().time()
        
        total_days = (end_utc.date() - start_utc.date()).days + 1
        processed_dates = set()
        latest_processed_date = None
        total_candles_received = 0
        
        def update_progress():
            elapsed_time = asyncio.get_event_loop().time() - start_time
            rps = request_count / elapsed_time if elapsed_time > 0 else 0
            cps = total_candles_received / elapsed_time if elapsed_time > 0 else 0
            
            processed_days = len(processed_dates)
            progress_pct = (processed_days / total_days) * 100 if total_days > 0 else 0
            
            latest_date_str = latest_processed_date.strftime("%Y-%m-%d") if latest_processed_date else "N/A"
            
            progress_msg = (f"\rğŸ“ˆ {symbol} {timeframe}: "
                          f"ìš”ì²­ {request_count:3d}/{max_requests} | "
                          f"RPS: {rps:4.1f} | "
                          f"CPS: {cps:5.0f} | "
                          f"ì§„í–‰: {processed_days:3d}/{total_days:3d}ì¼ ({progress_pct:5.1f}%) | "
                          f"ìµœì‹ : {latest_date_str}")
            
            print(progress_msg, end="", flush=True)
        
        try:
            update_progress()
            
            while current_start < end_utc and request_count < max_requests:
                params = {
                    "symbol": symbol,
                    "interval": timeframe,
                    "startTime": int(current_start.timestamp() * 1000),
                    "endTime": int(end_utc.timestamp() * 1000),
                    "limit": self.max_candles_per_request
                }
                
                request_count += 1
                
                # ì¬ì‹œë„ ë¡œì§
                max_retries = 3
                retry_delay = 1
                candles = None
                
                for attempt in range(max_retries):
                    try:
                        async with session.get(url, params=params) as response:
                            response.raise_for_status()
                            candles = await response.json()
                            break
                    except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                        if attempt < max_retries - 1:
                            await asyncio.sleep(retry_delay * (attempt + 1))
                        else:
                            raise e
                
                if not candles:
                    break
                
                # ë°ì´í„° íŒŒì‹± ë° í•„í„°ë§
                valid_candles = []
                latest_in_batch = None
                
                for kline in candles:
                    candle_time = datetime.fromtimestamp(kline[0] / 1000, tz=timezone.utc)
                    
                    if start_utc <= candle_time <= end_utc:
                        valid_candles.append(kline)
                        processed_dates.add(candle_time.date())
                        if latest_in_batch is None or candle_time > latest_in_batch:
                            latest_in_batch = candle_time
                
                if valid_candles:
                    all_candles.extend(valid_candles)
                    total_candles_received += len(valid_candles)
                    
                    if latest_in_batch:
                        latest_processed_date = latest_in_batch
                        current_start = latest_in_batch + timedelta(milliseconds=1)
                    
                    update_progress()
                else:
                    break
                
                await asyncio.sleep(self.rate_limit_delay)
            
            if request_count >= max_requests:
                print(f"\nâš ï¸  ìµœëŒ€ ìš”ì²­ ìˆ˜ ({max_requests}) ë„ë‹¬")
            else:
                print()
                    
        finally:
            elapsed_time = asyncio.get_event_loop().time() - start_time
            final_rps = request_count / elapsed_time if elapsed_time > 0 else 0
            final_cps = total_candles_received / elapsed_time if elapsed_time > 0 else 0
            processed_days = len(processed_dates)
            
            print(f"âœ… {symbol} {timeframe} ì™„ë£Œ: "
                  f"{total_candles_received}ê°œ ìº”ë“¤, {processed_days}/{total_days}ì¼, "
                  f"í‰ê·  {final_rps:.1f} RPS, {final_cps:.0f} CPS, "
                  f"{elapsed_time:.1f}ì´ˆ ì†Œìš”")
        
        # DataFrame ë³€í™˜
        if all_candles:
            df_data = []
            for kline in all_candles:
                parsed_candle = self._parse_binance_kline(kline, symbol)
                df_data.append(parsed_candle)
            
            result_df = pl.DataFrame(df_data).sort("timestamp")
            # ë¹ˆê°’ ì±„ìš°ê¸°
            result_df = self._fill_missing_data(result_df)
            return result_df
        
        # ë¹ˆ DataFrame ë°˜í™˜
        return pl.DataFrame(schema={
            "timestamp": pl.Datetime,
            "symbol": pl.Utf8,
            "open": pl.Float64,
            "high": pl.Float64,
            "low": pl.Float64,
            "close": pl.Float64,
            "volume": pl.Float64
        })
    
    async def _fetch_raw_data(
        self,
        symbols: List[str],
        start: datetime,
        end: datetime,
        timeframe: str
    ) -> pl.DataFrame:
        """ì›ì‹œ ë°ì´í„° ì¡°íšŒ - 3ë‹¨ê³„ ì‹œìŠ¤í…œ (ìºì‹œ -> DB -> API)"""
        if timeframe in self.SUPPORTED_API_TIMEFRAMES:
            api_timeframe = timeframe
        else:
            api_timeframe = "1m"
        
        start_utc = self._to_utc(start)
        end_utc = self._to_utc(end)
        
        all_data = []
        
        for symbol in symbols:
            # 1ë‹¨ê³„: Parquet ìºì‹œ í™•ì¸
            cached_data = self.cache_manager.load_cache(
                self.PROVIDER_NAME, symbol, api_timeframe, start_utc, end_utc
            )
            
            if cached_data is not None:
                all_data.append(cached_data)
                continue
            
            # 2ë‹¨ê³„: DBì—ì„œ ê¸°ì¡´ ë°ì´í„° í™•ì¸ ë° ëˆ„ë½ ê¸°ê°„ ê³„ì‚°
            existing_ranges = self._get_existing_data_ranges(
                symbol, api_timeframe, start_utc, end_utc
            )
            
            missing_periods = self._calculate_missing_periods(
                symbol, api_timeframe, start_utc, end_utc, existing_ranges
            )
            
            # 3ë‹¨ê³„: ëˆ„ë½ëœ ë°ì´í„°ë§Œ APIì—ì„œ ì¡°íšŒ
            for period in missing_periods:
                print(f"ğŸ”„ {symbol} {api_timeframe} API ì¡°íšŒ: "
                      f"{period['start'].strftime('%Y-%m-%d %H:%M')} ~ "
                      f"{period['end'].strftime('%Y-%m-%d %H:%M')}")
                
                try:
                    new_data = await self._fetch_candles_from_api(
                        symbol, api_timeframe, period["start"], period["end"]
                    )
                    
                    if not new_data.is_empty():
                        # ë¹ˆê°’ ì±„ìš°ê¸°
                        new_data = self._fill_missing_data(new_data)
                        
                        # DBì— ì €ì¥
                        self.db_manager.save_market_data(
                            self.PROVIDER_NAME, symbol, api_timeframe, new_data
                        )
                        
                        print(f"ğŸ’¾ {symbol} {api_timeframe}: {len(new_data)}ê°œ ìº”ë“¤ DB ì €ì¥ ì™„ë£Œ")
                    
                except Exception as e:
                    print(f"âŒ {symbol} {api_timeframe} API ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # DBì—ì„œ ìš”ì²­ ë²”ìœ„ì˜ ëª¨ë“  ë°ì´í„° ë¡œë“œ
            db_data = self.db_manager.get_market_data(
                self.PROVIDER_NAME, symbol, api_timeframe, start_utc, end_utc
            )
            
            if not db_data.is_empty():
                # ìºì‹œ ì €ì¥ì€ ì‹¤ì œ ì¡°íšŒ ì‹œì—ë§Œ ìˆ˜í–‰ (ì„±ëŠ¥ ìµœì í™”)
                # self.cache_manager.save_cache(...)  # ì œê±°ë¨
                all_data.append(db_data)
        
        # ëª¨ë“  ì‹¬ë³¼ ë°ì´í„° ê²°í•©
        if all_data:
            combined_data = pl.concat(all_data)
            
            # ë¦¬ìƒ˜í”Œë§ì´ í•„ìš”í•œ ê²½ìš°
            if timeframe != api_timeframe:
                combined_data = TimeframeUtils.resample_data(
                    combined_data, api_timeframe, timeframe
                )
            
            return combined_data.sort(["symbol", "timestamp"])
        
        # ë¹ˆ DataFrame ë°˜í™˜
        return pl.DataFrame(schema={
            "timestamp": pl.Datetime,
            "symbol": pl.Utf8,
            "open": pl.Float64,
            "high": pl.Float64,
            "low": pl.Float64,
            "close": pl.Float64,
            "volume": pl.Float64
        })
    
    async def _fetch_raw_data_download_only(
        self,
        symbols: List[str],
        start: datetime,
        end: datetime,
        timeframe: str
    ) -> Dict[str, int]:
        """ë‹¤ìš´ë¡œë“œ ì „ìš© ì›ì‹œ ë°ì´í„° ì¡°íšŒ - ìºì‹œ ì €ì¥ ì—†ìŒ"""
        if timeframe in self.SUPPORTED_API_TIMEFRAMES:
            api_timeframe = timeframe
        else:
            api_timeframe = "1m"
        
        start_utc = self._to_utc(start)
        end_utc = self._to_utc(end)
        
        download_counts = {}
        
        for symbol in symbols:
            total_downloaded = 0
            
            # ë‹¤ìš´ë¡œë“œ ì „ìš©: ìºì‹œ í™•ì¸ ìƒëµ, DBì—ì„œ ì§ì ‘ í™•ì¸
            # DBì—ì„œ ê¸°ì¡´ ë°ì´í„° í™•ì¸ ë° ëˆ„ë½ ê¸°ê°„ ê³„ì‚°
            existing_ranges = self._get_existing_data_ranges(
                symbol, api_timeframe, start_utc, end_utc
            )
            
            missing_periods = self._calculate_missing_periods(
                symbol, api_timeframe, start_utc, end_utc, existing_ranges
            )
            
            # ëˆ„ë½ëœ ë°ì´í„°ë§Œ APIì—ì„œ ì¡°íšŒ (ìºì‹œ ì €ì¥ ì—†ìŒ)
            for period in missing_periods:
                print(f"ğŸ”„ {symbol} {api_timeframe} API ì¡°íšŒ: "
                      f"{period['start'].strftime('%Y-%m-%d %H:%M')} ~ "
                      f"{period['end'].strftime('%Y-%m-%d %H:%M')}")
                
                try:
                    new_data = await self._fetch_candles_from_api(
                        symbol, api_timeframe, period["start"], period["end"]
                    )
                    
                    if not new_data.is_empty():
                        # ë¹ˆê°’ ì±„ìš°ê¸°
                        new_data = self._fill_missing_data(new_data)
                        
                        # DBì—ë§Œ ì €ì¥ (ìºì‹œ ì €ì¥ ì•ˆí•¨)
                        saved_count = self.db_manager.save_market_data(
                            self.PROVIDER_NAME, symbol, api_timeframe, new_data
                        )
                        
                        total_downloaded += saved_count
                        print(f"ğŸ’¾ {symbol} {api_timeframe}: {saved_count}ê°œ ìº”ë“¤ DB ì €ì¥ ì™„ë£Œ")
                    
                except Exception as e:
                    print(f"âŒ {symbol} {api_timeframe} API ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    download_counts[symbol] = -1
                    continue
            
            # ê¸°ì¡´ + ìƒˆë¡œ ë‹¤ìš´ë¡œë“œëœ ë°ì´í„° ì´ ê°œìˆ˜
            if symbol not in download_counts or download_counts[symbol] != -1:
                total_existing = self.db_manager.get_data_count(
                    self.PROVIDER_NAME, symbol, api_timeframe
                ) if hasattr(self.db_manager, 'get_data_count') else 0
                
                download_counts[symbol] = total_existing
        
        return download_counts
    
    async def export_to_csv(
        self,
        symbols: List[str],
        start: datetime,
        end: datetime,
        timeframe: str = "1m",
        output_dir: str = "./binance_data"
    ) -> Dict[str, str]:
        """ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        exported_files = {}
        
        for symbol in symbols:
            print(f"ğŸ“„ {symbol} CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘...")
            
            try:
                # CSV ë‚´ë³´ë‚´ê¸°ìš© ë°ì´í„° ì¡°íšŒ (ìºì‹œ ì‚¬ìš©)
                data = await self._fetch_raw_data([symbol], start, end, timeframe)
                
                if data.is_empty():
                    print(f"âš ï¸ {symbol}: ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # CSV íŒŒì¼ëª… ìƒì„±
                start_str = start.strftime("%Y%m%d")
                end_str = end.strftime("%Y%m%d")
                filename = f"{symbol}_{timeframe}_{start_str}_{end_str}.csv"
                filepath = output_path / filename
                
                # CSVë¡œ ì €ì¥
                data.write_csv(str(filepath))
                exported_files[symbol] = str(filepath)
                
                print(f"âœ… {symbol}: {filepath} ì €ì¥ ì™„ë£Œ ({len(data)}ê°œ ìº”ë“¤)")
                
            except Exception as e:
                print(f"âŒ {symbol} CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        
        return exported_files
    
    async def download_all_usdt_symbols(
        self,
        start: datetime,
        end: datetime,
        timeframe: str = "1m",
        batch_size: int = 5
    ) -> Dict[str, int]:
        """ì „ì²´ USDT í˜ì–´ ì‹¬ë³¼ì˜ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        print("ğŸ” USDT í˜ì–´ ì‹¬ë³¼ ëª©ë¡ ì¡°íšŒ ì¤‘...")
        usdt_symbols = await self.get_usdt_symbols()
        
        if not usdt_symbols:
            print("âŒ USDT í˜ì–´ ì‹¬ë³¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"ğŸ“Š ì´ {len(usdt_symbols)}ê°œ USDT í˜ì–´ ë°œê²¬")
        print(f"ğŸ“… ê¸°ê°„: {start.strftime('%Y-%m-%d')} ~ {end.strftime('%Y-%m-%d')}")
        print(f"â° íƒ€ì„í”„ë ˆì„: {timeframe}")
        print(f"ğŸ”¢ ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        downloaded_counts = {}
        failed_symbols = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(usdt_symbols), batch_size):
            batch_symbols = usdt_symbols[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(usdt_symbols) + batch_size - 1) // batch_size
            
            print(f"\nğŸ”„ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘: {batch_symbols}")
            
            # ë°°ì¹˜ ë‚´ ì‹¬ë³¼ë“¤ì„ ìˆœì°¨ ì²˜ë¦¬ (ë™ì‹œ ì²˜ë¦¬ëŠ” API ì œí•œì— ê±¸ë¦´ ìˆ˜ ìˆìŒ)
            for symbol in batch_symbols:
                try:
                    print(f"\nğŸ“ˆ {symbol} ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
                    
                    # ë‹¤ìš´ë¡œë“œ ì „ìš© ë©”ì„œë“œ ì‚¬ìš© (ìºì‹œ ì €ì¥ ì—†ìŒ)
                    symbol_counts = await self._fetch_raw_data_download_only([symbol], start, end, timeframe)
                    
                    count = symbol_counts.get(symbol, 0)
                    if count > 0:
                        downloaded_counts[symbol] = count
                        print(f"âœ… {symbol}: {count}ê°œ ìº”ë“¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    elif count == 0:
                        print(f"âš ï¸ {symbol}: ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        downloaded_counts[symbol] = 0
                    else:
                        # ì´ë¯¸ ì‹¤íŒ¨ ì²˜ë¦¬ë¨
                        pass
                    
                except Exception as e:
                    print(f"âŒ {symbol} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                    failed_symbols.append(symbol)
                    downloaded_counts[symbol] = -1
            
            # ë°°ì¹˜ ê°„ ëŒ€ê¸° (API ì œí•œ ë°©ì§€)
            if i + batch_size < len(usdt_symbols):
                print(f"â±ï¸ ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ {self.rate_limit_delay * 10}ì´ˆ ëŒ€ê¸°...")
                await asyncio.sleep(self.rate_limit_delay * 10)
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ìš”ì•½:")
        print(f"âœ… ì„±ê³µ: {len([c for c in downloaded_counts.values() if c >= 0])}ê°œ ì‹¬ë³¼")
        print(f"âŒ ì‹¤íŒ¨: {len(failed_symbols)}ê°œ ì‹¬ë³¼")
        
        if failed_symbols:
            print(f"ì‹¤íŒ¨í•œ ì‹¬ë³¼ë“¤: {failed_symbols[:10]}{'...' if len(failed_symbols) > 10 else ''}")
        
        total_candles = sum(c for c in downloaded_counts.values() if c > 0)
        print(f"ğŸ“ˆ ì´ ë‹¤ìš´ë¡œë“œëœ ìº”ë“¤ ìˆ˜: {total_candles:,}ê°œ")
        
        return downloaded_counts
    
    def get_storage_info(self) -> Dict[str, Any]:
        """ì €ì¥ì†Œ ì •ë³´ ì¡°íšŒ"""
        return {
            "provider": self.PROVIDER_NAME,
            "db_info": {
                "db_path": str(self.db_manager.db_path),
                "provider": self.PROVIDER_NAME
            },
            "cache_info": self.cache_manager.get_cache_info() if hasattr(self.cache_manager, 'get_cache_info') else {}
        }
    
    def cleanup_storage(self, days: int = 30):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        self.cache_manager.clear_old_cache(days)
    
    def get_cache_info(self) -> Dict[str, Any]:
        """ìºì‹œ ì •ë³´ ì¡°íšŒ"""
        return self.cache_manager.get_cache_info()
    
    def clear_cache(self, symbol: Optional[str] = None, timeframe: Optional[str] = None):
        """ìºì‹œ ì •ë¦¬"""
        self.cache_manager.clear_cache_by_criteria(
            provider=self.PROVIDER_NAME, 
            symbol=symbol, 
            timeframe=timeframe
        ) 