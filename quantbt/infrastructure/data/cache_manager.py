"""
Parquet ê¸°ë°˜ ìºì‹œ ê´€ë¦¬ì

SQLiteì—ì„œ ì¡°íšŒëœ ë°ì´í„°ë¥¼ Parquet íŒŒì¼ë¡œ ìºì‹œí•˜ì—¬ ë¹ ë¥¸ ì¬ì¡°íšŒë¥¼ ì§€ì›í•˜ëŠ” í´ë˜ìŠ¤
"""

import hashlib
import sqlite3
from pathlib import Path
from datetime import datetime, timezone
from typing import Optional, List
import polars as pl

# config.pyì—ì„œ ì ˆëŒ€ê²½ë¡œ import
try:
    from ...config import CACHE_DIR, DB_PATH
except ImportError:
    # config.pyê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
    CACHE_DIR = "/home/lazydok/src/quantbt/data/cache/"
    DB_PATH = "/home/lazydok/src/quantbt/data/quantbt.db"


class CacheManager:
    """Parquet ê¸°ë°˜ ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self, cache_dir: str = None, db_path: str = None):
        if cache_dir is None:
            cache_dir = CACHE_DIR
        if db_path is None:
            db_path = DB_PATH
            
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.db_path = Path(db_path)
    
    def _generate_cache_key(self, provider: str, symbol: str, timeframe: str,
                           start_utc: datetime, end_utc: datetime) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ì‹œê°„ì„ ì •ê·œí™”í•˜ì—¬ ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•´ ê°™ì€ í‚¤ ìƒì„±
        start_str = start_utc.replace(tzinfo=None).isoformat()
        end_str = end_utc.replace(tzinfo=None).isoformat()
        
        key_string = f"{provider}:{symbol}:{timeframe}:{start_str}:{end_str}"
        return hashlib.sha256(key_string.encode()).hexdigest()[:16]  # 16ìë¦¬ í•´ì‹œ
    
    def _get_cache_file_path(self, cache_key: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        return self.cache_dir / f"{cache_key}.parquet"
    
    def has_cache(self, provider: str, symbol: str, timeframe: str,
                  start_utc: datetime, end_utc: datetime) -> bool:
        """ìºì‹œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        cache_key = self._generate_cache_key(provider, symbol, timeframe, start_utc, end_utc)
        cache_file = self._get_cache_file_path(cache_key)
        
        if not cache_file.exists():
            return False
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œë„ í™•ì¸
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT file_path FROM cache_metadata 
                WHERE cache_key = ? AND file_path = ?
            ''', (cache_key, str(cache_file)))
            
            return cursor.fetchone() is not None
    
    def save_cache(self, provider: str, symbol: str, timeframe: str,
                   start_utc: datetime, end_utc: datetime, data: pl.DataFrame, 
                   max_cache_size_mb: float = 100.0) -> bool:
        """ë°ì´í„°ë¥¼ Parquet ìºì‹œë¡œ ì €ì¥ (ìë™ í¬ê¸° ê´€ë¦¬ í¬í•¨)"""
        if data.height == 0:
            return False
        
        cache_key = self._generate_cache_key(provider, symbol, timeframe, start_utc, end_utc)
        cache_file = self._get_cache_file_path(cache_key)
        
        try:
            # ìºì‹œ ì €ì¥ ì „ í¬ê¸° í™•ì¸ ë° ìë™ ì •ë¦¬
            self._check_and_manage_cache_size(max_cache_size_mb)
            
            # Parquet íŒŒì¼ë¡œ ì €ì¥
            data.write_parquet(cache_file)
            
            # í–¥ìƒëœ ë©”íƒ€ë°ì´í„° ì €ì¥ (provider, symbol, timeframe í¬í•¨)
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    INSERT OR REPLACE INTO cache_metadata 
                    (cache_key, file_path, provider, symbol, timeframe, 
                     start_time_utc, end_time_utc, record_count, data_source)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    cache_key,
                    str(cache_file),
                    provider,
                    symbol,
                    timeframe,
                    start_utc.replace(tzinfo=None).isoformat(),
                    end_utc.replace(tzinfo=None).isoformat(),
                    data.height,
                    "api"  # ë°ì´í„° ì†ŒìŠ¤
                ))
                conn.commit()
            
            return True
            
        except Exception as e:
            print(f"âŒ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def _check_and_manage_cache_size(self, max_size_mb: float):
        """ìºì‹œ í¬ê¸° í™•ì¸ ë° ìë™ ê´€ë¦¬ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        try:
            # í˜„ì¬ ìºì‹œ í¬ê¸° ê³„ì‚°
            cache_files = list(self.cache_dir.glob("*.parquet"))
            current_size_mb = sum(f.stat().st_size for f in cache_files if f.exists()) / (1024 * 1024)
            
            if current_size_mb > max_size_mb:
                print(f"ğŸš¨ ìºì‹œ í¬ê¸° í•œê³„ ì´ˆê³¼: {current_size_mb:.1f}MB > {max_size_mb}MB")
                cleanup_result = self.auto_cleanup_by_size(max_size_mb * 0.8)  # 80%ê¹Œì§€ ì •ë¦¬
                
                if cleanup_result["success"]:
                    print(f"ğŸ§¹ ìë™ ì •ë¦¬ ì™„ë£Œ: {cleanup_result['space_freed_mb']:.1f}MB í™•ë³´")
                else:
                    print(f"âš ï¸ ìë™ ì •ë¦¬ ì‹¤íŒ¨: {cleanup_result['errors']}")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ í¬ê¸° ê´€ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def find_cache_by_criteria(self, provider: str = None, symbol: str = None, 
                              timeframe: str = None) -> List[dict]:
        """ì¡°ê±´ë³„ ìºì‹œ ê²€ìƒ‰"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                where_conditions = []
                params = []
                
                if provider:
                    where_conditions.append("provider = ?")
                    params.append(provider)
                
                if symbol:
                    where_conditions.append("symbol = ?")
                    params.append(symbol)
                
                if timeframe:
                    where_conditions.append("timeframe = ?")
                    params.append(timeframe)
                
                where_clause = ""
                if where_conditions:
                    where_clause = "WHERE " + " AND ".join(where_conditions)
                
                query = f'''
                    SELECT cache_key, file_path, provider, symbol, timeframe,
                           start_time_utc, end_time_utc, record_count,
                           created_at, last_accessed, data_source
                    FROM cache_metadata 
                    {where_clause}
                    ORDER BY created_at DESC
                '''
                
                cursor = conn.execute(query, params)
                columns = [description[0] for description in cursor.description]
                
                results = []
                for row in cursor.fetchall():
                    result_dict = dict(zip(columns, row))
                    results.append(result_dict)
                
                return results
                
        except Exception as e:
            print(f"âŒ ìºì‹œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def clear_cache_by_criteria(self, provider: str = None, symbol: str = None, 
                               timeframe: str = None) -> dict:
        """ì¡°ê±´ë³„ ìºì‹œ ì„ íƒ ì‚­ì œ (ê°œì„ ëœ ë²„ì „)"""
        result = {
            "files_deleted": 0,
            "metadata_deleted": 0,
            "errors": [],
            "success": False
        }
        
        try:
            # ì¡°ê±´ì— ë§ëŠ” ìºì‹œ ì°¾ê¸°
            matching_caches = self.find_cache_by_criteria(provider, symbol, timeframe)
            
            if not matching_caches:
                result["success"] = True
                print(f"ğŸ’¡ ì‚­ì œí•  ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤ (ì¡°ê±´: provider={provider}, symbol={symbol}, timeframe={timeframe})")
                return result
            
            with sqlite3.connect(self.db_path) as conn:
                deleted_keys = []
                
                for cache_entry in matching_caches:
                    cache_key = cache_entry['cache_key']
                    file_path = cache_entry['file_path']
                    
                    try:
                        # íŒŒì¼ ì‚­ì œ
                        cache_file = Path(file_path)
                        if cache_file.exists():
                            cache_file.unlink()
                            result["files_deleted"] += 1
                        
                        deleted_keys.append(cache_key)
                        
                    except Exception as e:
                        result["errors"].append(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {cache_key}: {str(e)}")
                
                # ë©”íƒ€ë°ì´í„° ì‚­ì œ
                if deleted_keys:
                    placeholders = ','.join(['?' for _ in deleted_keys])
                    cursor = conn.execute(f'''
                        DELETE FROM cache_metadata 
                        WHERE cache_key IN ({placeholders})
                    ''', deleted_keys)
                    result["metadata_deleted"] = cursor.rowcount
                    conn.commit()
            
            result["success"] = True
            
            # ì¡°ê±´ ì •ë³´ í¬í•¨í•œ ë©”ì‹œì§€ ì¶œë ¥
            condition_parts = []
            if provider:
                condition_parts.append(f"provider={provider}")
            if symbol:
                condition_parts.append(f"symbol={symbol}")
            if timeframe:
                condition_parts.append(f"timeframe={timeframe}")
            
            condition_str = ", ".join(condition_parts) if condition_parts else "ëª¨ë“  ì¡°ê±´"
            
            print(f"âœ… ì¡°ê±´ë³„ ìºì‹œ ì‚­ì œ ì™„ë£Œ ({condition_str}): "
                  f"{result['files_deleted']}ê°œ íŒŒì¼, {result['metadata_deleted']}ê°œ ë©”íƒ€ë°ì´í„° ì‚­ì œ")
            
        except Exception as e:
            result["errors"].append(f"ì¡°ê±´ë³„ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            result["success"] = False
        
        return result
    
    def load_cache(self, provider: str, symbol: str, timeframe: str,
                   start_utc: datetime, end_utc: datetime) -> Optional[pl.DataFrame]:
        """Parquet ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ - ë²”ìœ„ í¬í•¨ ë°©ì‹ìœ¼ë¡œ ê°œì„ """
        # 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ ì‹œë„
        cache_key = self._generate_cache_key(provider, symbol, timeframe, start_utc, end_utc)
        cache_file = self._get_cache_file_path(cache_key)
        
        if cache_file.exists():
            try:
                data = pl.read_parquet(cache_file)
                self._update_cache_access(cache_key)
                return data
            except Exception as e:
                self._delete_cache_file(cache_key)
        
        # 2ë‹¨ê³„: ë²”ìœ„ í¬í•¨ ìºì‹œ ê²€ìƒ‰ - í•´ì‹œ ê¸°ë°˜ ìºì‹œ í‚¤ë¡œëŠ” íŒ¨í„´ ë§¤ì¹­ ë¶ˆê°€
        # ì •í™•í•œ ë§¤ì¹­ë§Œ ì‚¬ìš© (ìºì‹œ í‚¤ê°€ í•´ì‹œê°’ì´ë¯€ë¡œ íŒ¨í„´ ê²€ìƒ‰ ë¶ˆê°€ëŠ¥)
        return None
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ì£¼ì„: ì´ ë¡œì§ì€ í•´ì‹œ ê¸°ë°˜ ìºì‹œ í‚¤ì—ì„œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŒ
                # í–¥í›„ ìºì‹œ í‚¤ êµ¬ì¡° ë³€ê²½ì‹œ ì¬í™œì„±í™” ê°€ëŠ¥
                cursor = conn.execute('''
                    SELECT cache_key, file_path, start_time_utc, end_time_utc
                    FROM cache_metadata 
                    WHERE start_time_utc <= ? 
                    AND end_time_utc >= ?
                    ORDER BY record_count DESC
                    LIMIT 1
                ''', (
                    start_utc.replace(tzinfo=None).isoformat(),
                    end_utc.replace(tzinfo=None).isoformat()
                ))
                
                result = cursor.fetchone()
                if result:
                    cached_key, file_path, cached_start, cached_end = result
                    cache_file_path = Path(file_path)
                    
                    if cache_file_path.exists():
                        data = pl.read_parquet(cache_file_path)
                        
                        # ìš”ì²­ëœ ë²”ìœ„ë¡œ í•„í„°ë§
                        filtered_data = data.filter(
                            (pl.col("timestamp") >= start_utc) & 
                            (pl.col("timestamp") <= end_utc)
                        )
                        
                        if filtered_data.height > 0:
                            self._update_cache_access(cached_key)
                            print(f"âš¡ ë²”ìœ„ ìºì‹œ íˆíŠ¸: {filtered_data.height}ê°œ ë ˆì½”ë“œ (ì „ì²´ {data.height}ê°œ ì¤‘)")
                            return filtered_data
        
        except Exception as e:
            print(f"âŒ ë²”ìœ„ ìºì‹œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        return None
    
    def _update_cache_access(self, cache_key: str):
        """ìºì‹œ ì ‘ê·¼ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    UPDATE cache_metadata 
                    SET last_accessed = CURRENT_TIMESTAMP
                    WHERE cache_key = ?
                ''', (cache_key,))
                conn.commit()
        except Exception:
            pass
    
    def _delete_cache_file(self, cache_key: str):
        """ìºì‹œ íŒŒì¼ ì‚­ì œ"""
        cache_file = self._get_cache_file_path(cache_key)
        
        try:
            if cache_file.exists():
                cache_file.unlink()
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œë„ ì‚­ì œ
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('DELETE FROM cache_metadata WHERE cache_key = ?', (cache_key,))
                conn.commit()
                
        except Exception:
            pass
    
    def cleanup_old_cache(self, days: int = 7):
        """ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ ì •ë¦¬"""
        cutoff_date = datetime.now(timezone.utc) - timezone.timedelta(days=days)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute('''
                SELECT cache_key, file_path FROM cache_metadata 
                WHERE last_accessed < ?
            ''', (cutoff_date.replace(tzinfo=None).isoformat(),))
            
            old_caches = cursor.fetchall()
            
            for cache_key, file_path in old_caches:
                try:
                    Path(file_path).unlink(missing_ok=True)
                    conn.execute('DELETE FROM cache_metadata WHERE cache_key = ?', (cache_key,))
                except Exception:
                    pass
            
            conn.commit()
            
        return len(old_caches)
    
    def clear_all_cache(self) -> dict:
        """ëª¨ë“  ìºì‹œ ì™„ì „ ì´ˆê¸°í™”"""
        result = {
            "files_deleted": 0,
            "metadata_deleted": 0,
            "errors": [],
            "success": False
        }
        
        try:
            # 1. ëª¨ë“  parquet íŒŒì¼ ì‚­ì œ
            cache_files = list(self.cache_dir.glob("*.parquet"))
            for cache_file in cache_files:
                try:
                    cache_file.unlink()
                    result["files_deleted"] += 1
                except Exception as e:
                    result["errors"].append(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {cache_file.name}: {str(e)}")
            
            # 2. ë©”íƒ€ë°ì´í„° ëª¨ë‘ ì‚­ì œ
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('DELETE FROM cache_metadata')
                result["metadata_deleted"] = cursor.rowcount
                conn.commit()
            
            result["success"] = True
            print(f"âœ… ìºì‹œ ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ: {result['files_deleted']}ê°œ íŒŒì¼, {result['metadata_deleted']}ê°œ ë©”íƒ€ë°ì´í„° ì‚­ì œ")
            
        except Exception as e:
            result["errors"].append(f"ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            result["success"] = False
        
        return result
    
    def clear_old_cache(self, days: int = 7) -> dict:
        """ì˜¤ë˜ëœ ìºì‹œë§Œ ì„ íƒ ì‚­ì œ"""
        from datetime import timedelta
        
        result = {
            "files_deleted": 0,
            "metadata_deleted": 0,
            "errors": [],
            "success": False
        }
        
        try:
            cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)
            
            with sqlite3.connect(self.db_path) as conn:
                # ì˜¤ë˜ëœ ìºì‹œ ì¡°íšŒ
                cursor = conn.execute('''
                    SELECT cache_key, file_path 
                    FROM cache_metadata 
                    WHERE last_accessed < ?
                ''', (cutoff_date.replace(tzinfo=None).isoformat(),))
                
                old_caches = cursor.fetchall()
                
                # íŒŒì¼ ì‚­ì œ
                for cache_key, file_path in old_caches:
                    try:
                        cache_file = Path(file_path)
                        if cache_file.exists():
                            cache_file.unlink()
                            result["files_deleted"] += 1
                    except Exception as e:
                        result["errors"].append(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {cache_key}: {str(e)}")
                
                # ë©”íƒ€ë°ì´í„° ì‚­ì œ
                cursor = conn.execute('''
                    DELETE FROM cache_metadata 
                    WHERE last_accessed < ?
                ''', (cutoff_date.replace(tzinfo=None).isoformat(),))
                
                result["metadata_deleted"] = cursor.rowcount
                conn.commit()
            
            result["success"] = True
            print(f"âœ… ì˜¤ë˜ëœ ìºì‹œ ì‚­ì œ ì™„ë£Œ ({days}ì¼ ì´ì „): {result['files_deleted']}ê°œ íŒŒì¼, {result['metadata_deleted']}ê°œ ë©”íƒ€ë°ì´í„° ì‚­ì œ")
            
        except Exception as e:
            result["errors"].append(f"ì˜¤ë˜ëœ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            result["success"] = False
        
        return result
    
    def rebuild_cache_index(self) -> dict:
        """ìºì‹œ ì¸ë±ìŠ¤ ì¬êµ¬ì„± (ê³ ì•„ íŒŒì¼ ì •ë¦¬)"""
        result = {
            "orphaned_files": 0,
            "missing_files": 0,
            "errors": [],
            "success": False
        }
        
        try:
            # íŒŒì¼ ì‹œìŠ¤í…œì˜ ëª¨ë“  parquet íŒŒì¼
            existing_files = {f.name for f in self.cache_dir.glob("*.parquet")}
            
            with sqlite3.connect(self.db_path) as conn:
                # ë©”íƒ€ë°ì´í„°ì˜ ëª¨ë“  íŒŒì¼
                cursor = conn.execute('SELECT cache_key, file_path FROM cache_metadata')
                metadata_entries = cursor.fetchall()
                
                metadata_files = set()
                missing_entries = []
                
                for cache_key, file_path in metadata_entries:
                    file_name = Path(file_path).name
                    metadata_files.add(file_name)
                    
                    # íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë©”íƒ€ë°ì´í„° ì—”íŠ¸ë¦¬
                    if file_name not in existing_files:
                        missing_entries.append(cache_key)
                
                # ê³ ì•„ íŒŒì¼ë“¤ (ë©”íƒ€ë°ì´í„°ì— ì—†ëŠ” íŒŒì¼ë“¤)
                orphaned_files = existing_files - metadata_files
                
                # ê³ ì•„ íŒŒì¼ ì‚­ì œ
                for orphaned_file in orphaned_files:
                    try:
                        (self.cache_dir / orphaned_file).unlink()
                        result["orphaned_files"] += 1
                    except Exception as e:
                        result["errors"].append(f"ê³ ì•„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {orphaned_file}: {str(e)}")
                
                # ëˆ„ë½ëœ íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„° ì‚­ì œ
                if missing_entries:
                    placeholders = ','.join(['?' for _ in missing_entries])
                    cursor = conn.execute(f'''
                        DELETE FROM cache_metadata 
                        WHERE cache_key IN ({placeholders})
                    ''', missing_entries)
                    result["missing_files"] = cursor.rowcount
                    conn.commit()
            
            result["success"] = True
            print(f"âœ… ìºì‹œ ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì™„ë£Œ: ê³ ì•„ íŒŒì¼ {result['orphaned_files']}ê°œ, ëˆ„ë½ ë©”íƒ€ë°ì´í„° {result['missing_files']}ê°œ ì •ë¦¬")
            
        except Exception as e:
            result["errors"].append(f"ìºì‹œ ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
            result["success"] = False
        
        return result
    
    def get_cache_health_report(self) -> dict:
        """ìºì‹œ ìƒíƒœ ê±´ê°•ì„± ë³´ê³ ì„œ (timeframe ì •ë³´ í¬í•¨)"""
        report = {
            "total_cache_files": 0,
            "total_metadata_entries": 0,
            "orphaned_files": 0,
            "missing_files": 0,
            "total_size_mb": 0.0,
            "oldest_cache_days": 0,
            "cache_efficiency": 0.0,
            "timeframe_stats": {},
            "provider_stats": {},
            "symbol_stats": {},
            "recommendations": []
        }
        
        try:
            # íŒŒì¼ ì‹œìŠ¤í…œ í†µê³„
            cache_files = list(self.cache_dir.glob("*.parquet"))
            report["total_cache_files"] = len(cache_files)
            
            total_size = sum(f.stat().st_size for f in cache_files if f.exists())
            report["total_size_mb"] = total_size / (1024 * 1024)
            
            existing_files = {f.name for f in cache_files}
            
            # ë©”íƒ€ë°ì´í„° í†µê³„
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('SELECT COUNT(*) FROM cache_metadata')
                report["total_metadata_entries"] = cursor.fetchone()[0]
                
                # timeframeë³„ í†µê³„
                cursor = conn.execute('''
                    SELECT timeframe, COUNT(*) as count, 
                           AVG(record_count) as avg_records,
                           SUM(record_count) as total_records
                    FROM cache_metadata 
                    WHERE timeframe IS NOT NULL
                    GROUP BY timeframe
                    ORDER BY count DESC
                ''')
                
                for row in cursor.fetchall():
                    timeframe, count, avg_records, total_records = row
                    report["timeframe_stats"][timeframe] = {
                        "cache_count": count,
                        "avg_records": int(avg_records) if avg_records else 0,
                        "total_records": int(total_records) if total_records else 0
                    }
                
                # providerë³„ í†µê³„
                cursor = conn.execute('''
                    SELECT provider, COUNT(*) as count
                    FROM cache_metadata 
                    WHERE provider IS NOT NULL
                    GROUP BY provider
                    ORDER BY count DESC
                ''')
                
                for row in cursor.fetchall():
                    provider, count = row
                    report["provider_stats"][provider] = count
                
                # symbolë³„ í†µê³„ (ìƒìœ„ 10ê°œ)
                cursor = conn.execute('''
                    SELECT symbol, COUNT(*) as count
                    FROM cache_metadata 
                    WHERE symbol IS NOT NULL
                    GROUP BY symbol
                    ORDER BY count DESC
                    LIMIT 10
                ''')
                
                for row in cursor.fetchall():
                    symbol, count = row
                    report["symbol_stats"][symbol] = count
                
                # ë©”íƒ€ë°ì´í„°ì™€ ì‹¤ì œ íŒŒì¼ ë§¤ì¹­ í™•ì¸
                cursor = conn.execute('SELECT cache_key, file_path FROM cache_metadata')
                metadata_entries = cursor.fetchall()
                
                metadata_files = set()
                missing_count = 0
                
                for cache_key, file_path in metadata_entries:
                    file_name = Path(file_path).name
                    metadata_files.add(file_name)
                    
                    if file_name not in existing_files:
                        missing_count += 1
                
                report["missing_files"] = missing_count
                report["orphaned_files"] = len(existing_files - metadata_files)
                
                # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ
                cursor = conn.execute('SELECT MIN(created_at) FROM cache_metadata')
                oldest_cache = cursor.fetchone()[0]
                
                if oldest_cache:
                    oldest_date = datetime.fromisoformat(oldest_cache)
                    report["oldest_cache_days"] = (datetime.now() - oldest_date).days
            
            # ìºì‹œ íš¨ìœ¨ì„± ê³„ì‚°
            if report["total_metadata_entries"] > 0:
                report["cache_efficiency"] = (
                    (report["total_metadata_entries"] - report["missing_files"]) / 
                    report["total_metadata_entries"] * 100
                )
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            if report["orphaned_files"] > 0:
                report["recommendations"].append(f"ê³ ì•„ íŒŒì¼ {report['orphaned_files']}ê°œ ì •ë¦¬ í•„ìš”")
            
            if report["missing_files"] > 0:
                report["recommendations"].append(f"ëˆ„ë½ëœ íŒŒì¼ ë©”íƒ€ë°ì´í„° {report['missing_files']}ê°œ ì •ë¦¬ í•„ìš”")
            
            if report["total_size_mb"] > 1000:  # 1GB ì´ìƒ
                report["recommendations"].append("ìºì‹œ í¬ê¸°ê°€ í½ë‹ˆë‹¤. ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
            
            if report["oldest_cache_days"] > 30:
                report["recommendations"].append("30ì¼ ì´ìƒ ëœ ìºì‹œê°€ ìˆìŠµë‹ˆë‹¤. ì •ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
            
            if report["cache_efficiency"] < 90:
                report["recommendations"].append("ìºì‹œ íš¨ìœ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ ì¬êµ¬ì„±ì„ ê¶Œì¥í•©ë‹ˆë‹¤")
            
            # timeframeë³„ ê¶Œì¥ì‚¬í•­
            if "1m" in report["timeframe_stats"] and "1d" in report["timeframe_stats"]:
                min_caches = report["timeframe_stats"]["1m"]["cache_count"]
                daily_caches = report["timeframe_stats"]["1d"]["cache_count"]
                if min_caches > daily_caches * 10:  # 1ë¶„ ìºì‹œê°€ ë„ˆë¬´ ë§ì€ ê²½ìš°
                    report["recommendations"].append("1ë¶„ ë°ì´í„° ìºì‹œê°€ ë§ìŠµë‹ˆë‹¤. ì˜¤ë˜ëœ 1ë¶„ ìºì‹œ ì •ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
        
        except Exception as e:
            report["error"] = str(e)
        
        return report

    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        stats = {
            "total_files": 0,
            "total_size_mb": 0.0,
            "symbols": {},
            "oldest_cache": None,
            "newest_cache": None
        }
        
        try:
            # íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ìºì‹œ íŒŒì¼ í†µê³„
            cache_files = list(self.cache_dir.glob("*.parquet"))
            stats["total_files"] = len(cache_files)
            
            total_size = sum(f.stat().st_size for f in cache_files if f.exists())
            stats["total_size_mb"] = total_size / (1024 * 1024)
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ìƒì„¸ í†µê³„
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('''
                    SELECT 
                        MIN(created_at) as oldest,
                        MAX(created_at) as newest,
                        COUNT(*) as meta_count
                    FROM cache_metadata
                ''')
                
                result = cursor.fetchone()
                if result and result[0]:
                    stats["oldest_cache"] = result[0]
                    stats["newest_cache"] = result[1]
                    stats["metadata_entries"] = result[2]
        
        except Exception as e:
            stats["error"] = str(e)
        
        return stats
    
    def auto_cleanup_by_size(self, max_size_mb: float = 100.0) -> dict:
        """ìºì‹œ í¬ê¸° ê¸°ë°˜ ìë™ ì •ë¦¬ - í¬ê¸° ì´ˆê³¼ì‹œ ì˜¤ë˜ëœ ìˆœì„œë¡œ ì‚­ì œ"""
        result = {
            "initial_size_mb": 0.0,
            "final_size_mb": 0.0,
            "files_deleted": 0,
            "metadata_deleted": 0,
            "space_freed_mb": 0.0,
            "success": False,
            "errors": []
        }
        
        try:
            # í˜„ì¬ ìºì‹œ í¬ê¸° ê³„ì‚°
            cache_files = list(self.cache_dir.glob("*.parquet"))
            initial_size = sum(f.stat().st_size for f in cache_files if f.exists())
            result["initial_size_mb"] = initial_size / (1024 * 1024)
            
            if result["initial_size_mb"] <= max_size_mb:
                result["success"] = True
                result["final_size_mb"] = result["initial_size_mb"]
                return result
            
            print(f"ğŸ“¦ ìºì‹œ í¬ê¸° í™•ì¸: {result['initial_size_mb']:.1f}MB (í•œê³„: {max_size_mb}MB)")
            print(f"ğŸ§¹ í¬ê¸° ì´ˆê³¼ë¡œ ì˜¤ë˜ëœ ìºì‹œë¶€í„° ìë™ ì •ë¦¬ ì‹œì‘...")
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì˜¤ë˜ëœ ìˆœì„œë¡œ ìºì‹œ ì¡°íšŒ
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('''
                    SELECT cache_key, file_path, created_at, record_count
                    FROM cache_metadata 
                    ORDER BY last_accessed ASC, created_at ASC
                ''')
                
                old_caches = cursor.fetchall()
                
                current_size = result["initial_size_mb"]
                deleted_keys = []
                
                for cache_key, file_path, created_at, record_count in old_caches:
                    if current_size <= max_size_mb:
                        break
                    
                    try:
                        # íŒŒì¼ í¬ê¸° í™•ì¸ í›„ ì‚­ì œ
                        cache_file = Path(file_path)
                        if cache_file.exists():
                            file_size_mb = cache_file.stat().st_size / (1024 * 1024)
                            cache_file.unlink()
                            current_size -= file_size_mb
                            result["files_deleted"] += 1
                            result["space_freed_mb"] += file_size_mb
                            
                            print(f"   ğŸ—‘ï¸ ì‚­ì œ: {cache_key} (í¬ê¸°: {file_size_mb:.1f}MB, ë ˆì½”ë“œ: {record_count:,}ê°œ)")
                        
                        deleted_keys.append(cache_key)
                        
                    except Exception as e:
                        result["errors"].append(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {cache_key}: {str(e)}")
                
                # ë©”íƒ€ë°ì´í„° ì‚­ì œ
                if deleted_keys:
                    placeholders = ','.join(['?' for _ in deleted_keys])
                    cursor = conn.execute(f'''
                        DELETE FROM cache_metadata 
                        WHERE cache_key IN ({placeholders})
                    ''', deleted_keys)
                    result["metadata_deleted"] = cursor.rowcount
                    conn.commit()
            
            # ìµœì¢… í¬ê¸° ê³„ì‚°
            remaining_files = list(self.cache_dir.glob("*.parquet"))
            final_size = sum(f.stat().st_size for f in remaining_files if f.exists())
            result["final_size_mb"] = final_size / (1024 * 1024)
            
            result["success"] = True
            print(f"âœ… í¬ê¸° ê¸°ë°˜ ì •ë¦¬ ì™„ë£Œ: {result['files_deleted']}ê°œ íŒŒì¼ ì‚­ì œ, "
                  f"{result['space_freed_mb']:.1f}MB í™•ë³´ "
                  f"({result['initial_size_mb']:.1f}MB â†’ {result['final_size_mb']:.1f}MB)")
            
        except Exception as e:
            result["errors"].append(f"í¬ê¸° ê¸°ë°˜ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
            result["success"] = False
        
        return result
    
    def check_and_cleanup_orphans(self) -> dict:
        """ê³ ì•„ íŒŒì¼ ì •ë¦¬ ì „ìš© ë©”ì„œë“œ"""
        result = {
            "orphaned_files_deleted": 0,
            "orphaned_metadata_deleted": 0,
            "success": False,
            "errors": [],
            "orphaned_files_list": [],
            "orphaned_metadata_list": []
        }
        
        try:
            print("ğŸ” ê³ ì•„ íŒŒì¼ ë° ë©”íƒ€ë°ì´í„° ê²€ì‚¬ ì‹œì‘...")
            
            # íŒŒì¼ ì‹œìŠ¤í…œì˜ ëª¨ë“  parquet íŒŒì¼
            existing_files = {}
            for f in self.cache_dir.glob("*.parquet"):
                existing_files[f.name] = f
            
            with sqlite3.connect(self.db_path) as conn:
                # ë©”íƒ€ë°ì´í„°ì˜ ëª¨ë“  íŒŒì¼
                cursor = conn.execute('SELECT cache_key, file_path FROM cache_metadata')
                metadata_entries = cursor.fetchall()
                
                metadata_files = set()
                orphaned_metadata_keys = []
                
                for cache_key, file_path in metadata_entries:
                    file_name = Path(file_path).name
                    metadata_files.add(file_name)
                    
                    # íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë©”íƒ€ë°ì´í„° ì—”íŠ¸ë¦¬
                    if file_name not in existing_files:
                        orphaned_metadata_keys.append(cache_key)
                        result["orphaned_metadata_list"].append({
                            "cache_key": cache_key,
                            "file_path": file_path
                        })
                
                # ê³ ì•„ íŒŒì¼ë“¤ (ë©”íƒ€ë°ì´í„°ì— ì—†ëŠ” íŒŒì¼ë“¤)
                orphaned_file_names = set(existing_files.keys()) - metadata_files
                
                if orphaned_file_names:
                    print(f"ğŸ—‘ï¸ ê³ ì•„ íŒŒì¼ {len(orphaned_file_names)}ê°œ ë°œê²¬:")
                    for file_name in orphaned_file_names:
                        file_path = existing_files[file_name]
                        file_size_mb = file_path.stat().st_size / (1024 * 1024)
                        result["orphaned_files_list"].append({
                            "file_name": file_name,
                            "file_path": str(file_path),
                            "size_mb": file_size_mb
                        })
                        print(f"   ğŸ“„ {file_name} (í¬ê¸°: {file_size_mb:.1f}MB)")
                
                if orphaned_metadata_keys:
                    print(f"ğŸ—ƒï¸ ê³ ì•„ ë©”íƒ€ë°ì´í„° {len(orphaned_metadata_keys)}ê°œ ë°œê²¬:")
                    for key in orphaned_metadata_keys[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                        print(f"   ğŸ”‘ {key}")
                    if len(orphaned_metadata_keys) > 5:
                        print(f"   ... (+{len(orphaned_metadata_keys) - 5}ê°œ ë”)")
                
                # ê³ ì•„ íŒŒì¼ ì‚­ì œ
                for file_name in orphaned_file_names:
                    try:
                        existing_files[file_name].unlink()
                        result["orphaned_files_deleted"] += 1
                    except Exception as e:
                        result["errors"].append(f"ê³ ì•„ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {file_name}: {str(e)}")
                
                # ê³ ì•„ ë©”íƒ€ë°ì´í„° ì‚­ì œ
                if orphaned_metadata_keys:
                    placeholders = ','.join(['?' for _ in orphaned_metadata_keys])
                    cursor = conn.execute(f'''
                        DELETE FROM cache_metadata 
                        WHERE cache_key IN ({placeholders})
                    ''', orphaned_metadata_keys)
                    result["orphaned_metadata_deleted"] = cursor.rowcount
                    conn.commit()
            
            result["success"] = True
            
            if result["orphaned_files_deleted"] > 0 or result["orphaned_metadata_deleted"] > 0:
                print(f"âœ… ê³ ì•„ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: "
                      f"íŒŒì¼ {result['orphaned_files_deleted']}ê°œ, "
                      f"ë©”íƒ€ë°ì´í„° {result['orphaned_metadata_deleted']}ê°œ ì‚­ì œ")
            else:
                print("âœ… ê³ ì•„ íŒŒì¼ ì—†ìŒ - ìºì‹œê°€ ì •ìƒ ìƒíƒœì…ë‹ˆë‹¤")
            
        except Exception as e:
            result["errors"].append(f"ê³ ì•„ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
            result["success"] = False
        
        return result 