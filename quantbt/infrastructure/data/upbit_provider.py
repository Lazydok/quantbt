"""
ì—…ë¹„íŠ¸ ë°ì´í„° í”„ë¡œë°”ì´ë” - SQLite + Parquet ê¸°ë°˜

ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œëœ ë©”ì¸ í”„ë¡œë°”ì´ë”
"""

import asyncio
import aiohttp
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime, timezone, timedelta
from zoneinfo import ZoneInfo
import polars as pl

from ...core.interfaces.data_provider import DataProviderBase
from ...core.utils.timeframe import TimeframeUtils
from ...config import DB_PATH, CACHE_DIR
from .database_manager import DatabaseManager
from .cache_manager import CacheManager


class UpbitDataProvider(DataProviderBase):
    """ì—…ë¹„íŠ¸ API ê¸°ë°˜ ë°ì´í„° ì œê³µì - SQLite + Parquet"""
    
    SUPPORTED_API_TIMEFRAMES = ["1m", "1d"]
    PROVIDER_NAME = "upbit"
    
    def __init__(
        self,
        db_path: str = None,
        cache_dir: str = None,
        rate_limit_delay: float = 0.1,
        max_candles_per_request: int = 200
    ):
        super().__init__("UpbitDataProvider")
        self.base_url = "https://api.upbit.com/v1"
        self.rate_limit_delay = rate_limit_delay
        self.max_candles_per_request = max_candles_per_request
        
        # config.pyì—ì„œ ì ˆëŒ€ê²½ë¡œ ì‚¬ìš©
        if db_path is None:
            db_path = DB_PATH
        if cache_dir is None:
            cache_dir = CACHE_DIR
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë° ìºì‹œ ë§¤ë‹ˆì €
        self.db_manager = DatabaseManager(db_path)
        self.cache_manager = CacheManager(cache_dir, db_path)
        
        # HTTP ì„¸ì…˜
        self._session: Optional[aiohttp.ClientSession] = None
        

    
    async def _get_session(self) -> aiohttp.ClientSession:
        """HTTP ì„¸ì…˜ ë°˜í™˜ - íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ì„¤ì • ê°•í™”"""
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(
                    total=10,  # ì „ì²´ ìš”ì²­ 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                    connect=3,  # ì—°ê²° 3ì´ˆ íƒ€ì„ì•„ì›ƒ
                    sock_read=5  # ì†Œì¼“ ì½ê¸° 5ì´ˆ íƒ€ì„ì•„ì›ƒ
                ),
                headers={
                    "accept": "application/json",
                    "user-agent": "QuantBT/1.0"
                },
                connector=aiohttp.TCPConnector(
                    limit=10,  # ë™ì‹œ ì—°ê²° ì œí•œ
                    ttl_dns_cache=300,  # DNS ìºì‹œ 5ë¶„
                    use_dns_cache=True
                )
            )
        return self._session
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self._session and not self._session.closed:
            await self._session.close()
            self._session = None
    
    def _normalize_timezone(self, dt: datetime) -> datetime:
        """datetimeì„ KSTë¡œ ì •ê·œí™”"""
        if dt.tzinfo is None:
            return dt.replace(tzinfo=ZoneInfo("Asia/Seoul"))
        else:
            return dt.astimezone(ZoneInfo("Asia/Seoul"))
    
    def _to_utc(self, dt: datetime) -> datetime:
        """KST datetimeì„ UTCë¡œ ë³€í™˜"""
        kst_dt = self._normalize_timezone(dt)
        return kst_dt.astimezone(timezone.utc)
    
    def _get_existing_data_ranges(
        self, 
        symbol: str, 
        timeframe: str, 
        start_utc: datetime, 
        end_utc: datetime
    ) -> List[Dict[str, datetime]]:
        """DBì—ì„œ ê¸°ì¡´ ë°ì´í„°ì˜ ì‹¤ì œ ë²”ìœ„ë“¤ì„ ì¡°íšŒ
        
        Args:
            symbol: ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„
            start_utc: ìš”ì²­ ì‹œì‘ ì‹œê°„ (UTC)
            end_utc: ìš”ì²­ ì¢…ë£Œ ì‹œê°„ (UTC)
            
        Returns:
            ê¸°ì¡´ ë°ì´í„° ë²”ìœ„ ë¦¬ìŠ¤íŠ¸ [{"start": datetime, "end": datetime}, ...]
        """
        try:
            import sqlite3
            ranges = []
            
            with sqlite3.connect(self.db_manager.db_path) as conn:
                # ìš”ì²­ ê¸°ê°„ ë‚´ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì‹œê°„ìˆœ ì¡°íšŒ
                cursor = conn.execute('''
                    SELECT timestamp_utc 
                    FROM market_data 
                    WHERE provider = ? AND symbol = ? AND timeframe = ?
                    AND timestamp_utc >= ? AND timestamp_utc <= ?
                    ORDER BY timestamp_utc
                ''', (
                    self.PROVIDER_NAME, 
                    symbol, 
                    timeframe,
                    start_utc.isoformat(),
                    end_utc.isoformat()
                ))
                
                timestamps = []
                for row in cursor.fetchall():
                    timestamp_str = row[0]
                    if timestamp_str.endswith("Z"):
                        timestamp_str = timestamp_str.replace("Z", "+00:00")
                    timestamp = datetime.fromisoformat(timestamp_str)
                    # UTC íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
                    if timestamp.tzinfo is None:
                        timestamp = timestamp.replace(tzinfo=timezone.utc)
                    timestamps.append(timestamp)
                
                if not timestamps:
                    return ranges
                
                # ì—°ì†ëœ ë°ì´í„° êµ¬ê°„ì„ ì°¾ì•„ì„œ ë²”ìœ„ë¡œ ê·¸ë£¹í•‘
                if timeframe == "1m":
                    gap_threshold = timedelta(minutes=10)  # 10ë¶„ ì´ìƒ ê°„ê²©ì´ë©´ ìƒˆë¡œìš´ êµ¬ê°„ (ê¸°ì¡´: 2ë¶„)
                elif timeframe == "1d":
                    gap_threshold = timedelta(days=2)  # 2ì¼ ì´ìƒ ê°„ê²©ì´ë©´ ìƒˆë¡œìš´ êµ¬ê°„
                else:
                    gap_threshold = timedelta(hours=2)  # ê¸°ë³¸ê°’: 2ì‹œê°„
                
                current_start = timestamps[0]
                current_end = timestamps[0]
                
                for i in range(1, len(timestamps)):
                    current_ts = timestamps[i]
                    
                    # ì´ì „ ì‹œê°„ê³¼ì˜ ê°„ê²© í™•ì¸
                    if current_ts - current_end <= gap_threshold:
                        # ì—°ì†ëœ ë°ì´í„° - í˜„ì¬ êµ¬ê°„ í™•ì¥
                        current_end = current_ts
                    else:
                        # ê°„ê²©ì´ ìˆìŒ - í˜„ì¬ êµ¬ê°„ ì™„ë£Œí•˜ê³  ìƒˆ êµ¬ê°„ ì‹œì‘
                        ranges.append({
                            "start": current_start,
                            "end": current_end
                        })
                        current_start = current_ts
                        current_end = current_ts
                
                # ë§ˆì§€ë§‰ êµ¬ê°„ ì¶”ê°€
                ranges.append({
                    "start": current_start,
                    "end": current_end
                })
                
            return ranges
            
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ë²”ìœ„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def _calculate_missing_periods(
        self,
        symbol: str,
        timeframe: str,
        requested_start: datetime,
        requested_end: datetime,
        existing_ranges: List[Dict[str, datetime]]
    ) -> List[Dict[str, datetime]]:
        """ìš”ì²­ ê¸°ê°„ì—ì„œ ê¸°ì¡´ ë°ì´í„° ë²”ìœ„ë¥¼ ì œì™¸í•œ ë¹ˆ ê¸°ê°„ë“¤ì„ ê³„ì‚°
        
        Args:
            symbol: ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„  
            requested_start: ìš”ì²­ ì‹œì‘ ì‹œê°„
            requested_end: ìš”ì²­ ì¢…ë£Œ ì‹œê°„
            existing_ranges: ê¸°ì¡´ ë°ì´í„° ë²”ìœ„ë“¤
            
        Returns:
            ë¹ˆ ê¸°ê°„ ë¦¬ìŠ¤íŠ¸ [{"start": datetime, "end": datetime}, ...]
        """
        if not existing_ranges:
            # ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì „ì²´ ê¸°ê°„ì´ ë¹ˆ ê¸°ê°„
            return [{
                "start": requested_start,
                "end": requested_end
            }]
        
        missing_periods = []
        
        # ê¸°ì¡´ ë²”ìœ„ë“¤ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_ranges = sorted(existing_ranges, key=lambda x: x["start"])
        
        # ì‹œê°„ ë¹„êµë¥¼ ìœ„í•´ ëª¨ë“  ì‹œê°„ì„ UTCë¡œ í†µì¼
        if requested_start.tzinfo is None:
            requested_start = requested_start.replace(tzinfo=timezone.utc)
        if requested_end.tzinfo is None:
            requested_end = requested_end.replace(tzinfo=timezone.utc)
        
        current_time = requested_start
        
        for existing_range in sorted_ranges:
            range_start = existing_range["start"]
            range_end = existing_range["end"]
            
            # ê¸°ì¡´ ë²”ìœ„ ì‹œê°„ë“¤ë„ UTCë¡œ í†µì¼
            if range_start.tzinfo is None:
                range_start = range_start.replace(tzinfo=timezone.utc)
            if range_end.tzinfo is None:
                range_end = range_end.replace(tzinfo=timezone.utc)
            
            # í˜„ì¬ ì‹œê°„ê³¼ ê¸°ì¡´ ë²”ìœ„ ì‹œì‘ ì‚¬ì´ì— ë¹ˆ ê¸°ê°„ì´ ìˆëŠ”ì§€ í™•ì¸
            if current_time < range_start:
                missing_periods.append({
                    "start": current_time,
                    "end": range_start
                })
            
            # ë‹¤ìŒ í™•ì¸ ì‹œì ì„ ê¸°ì¡´ ë²”ìœ„ ëìœ¼ë¡œ ì´ë™
            current_time = max(current_time, range_end)
        
        # ë§ˆì§€ë§‰ ê¸°ì¡´ ë²”ìœ„ ì´í›„ë¶€í„° ìš”ì²­ ì¢…ë£Œê¹Œì§€ ë¹ˆ ê¸°ê°„ í™•ì¸
        if current_time < requested_end:
            missing_periods.append({
                "start": current_time,
                "end": requested_end
            })
        
        return missing_periods
    
    def _load_available_symbols(self) -> List[str]:
        """ì—…ë¹„íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¬ë³¼ ë¡œë“œ"""
        # ì£¼ìš” ì•”í˜¸í™”í ì‹¬ë³¼ë“¤ (ì‹¤ì œë¡œëŠ” APIì—ì„œ ì¡°íšŒí•  ìˆ˜ ìˆì§€ë§Œ ë‹¨ìˆœí™”)
        return [
            "KRW-BTC", "KRW-ETH", "KRW-XRP", "KRW-ADA", "KRW-AVAX",
            "KRW-LINK", "KRW-DOT", "KRW-MATIC", "KRW-SOL", "KRW-DOGE",
            "KRW-SHIB", "KRW-ATOM", "KRW-NEAR", "KRW-APT", "KRW-SUI"
        ]
    
    def validate_timeframe(self, timeframe: str) -> bool:
        """ëª¨ë“  íƒ€ì„í”„ë ˆì„ ì§€ì› (ë¦¬ìƒ˜í”Œë§ ì²˜ë¦¬)"""
        return TimeframeUtils.validate_timeframe(timeframe)
    
    def validate_date_range(self, start: datetime, end: datetime) -> bool:
        """ë‚ ì§œ ë²”ìœ„ ìœ íš¨ì„± ê²€ì¦ - íƒ€ì„ì¡´ ì²˜ë¦¬"""
        # íƒ€ì„ì¡´ ì •ê·œí™”
        start_normalized = self._normalize_timezone(start)
        end_normalized = self._normalize_timezone(end)
        now_kst = datetime.now(ZoneInfo("Asia/Seoul"))
        
        return start_normalized <= end_normalized and start_normalized <= now_kst
    
    async def _check_network_connection(self) -> bool:
        """ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            session = await self._get_session()
            # ì—…ë¹„íŠ¸ ì„œë²„ ìƒíƒœ í™•ì¸
            async with session.get(f"{self.base_url}/market/all") as response:
                return response.status == 200
        except Exception:
            return False
    
    async def _fetch_candles_from_api(
        self,
        symbol: str,
        timeframe: str,
        start: datetime,
        end: datetime
    ) -> pl.DataFrame:
        """ì—…ë¹„íŠ¸ APIì—ì„œ ìº”ë“¤ ë°ì´í„° ì¡°íšŒ"""
        if timeframe not in self.SUPPORTED_API_TIMEFRAMES:
            raise ValueError(f"API timeframe not supported: {timeframe}")
        
        # KSTë¡œ ì •ê·œí™” (ì—…ë¹„íŠ¸ APIëŠ” KST ì‹œê°„ ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥)
        start_kst = self._normalize_timezone(start)
        end_kst = self._normalize_timezone(end)
        
        session = await self._get_session()
        all_candles = []
        
        url = f"{self.base_url}/candles/minutes/1" if timeframe == "1m" else f"{self.base_url}/candles/days"
        params_base = {"market": symbol}
        current_end = end_kst
        
        # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ ìš”ì²­ ìˆ˜ ì œí•œ
        max_requests = 10000  # ìµœëŒ€ 10000 ìš”ì²­
        request_count = 0
        start_time = asyncio.get_event_loop().time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        
        # ì¼ì ê¸°ë°˜ ì§„í–‰ë¥  ì¶”ì 
        total_days = (end_kst.date() - start_kst.date()).days + 1
        processed_dates = set()  # ì²˜ë¦¬ëœ ë‚ ì§œë“¤ì„ ì¶”ì 
        latest_processed_date = None
        
        # ì§„í–‰ë¥  ë¡œê¹…ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
        total_candles_received = 0
        
        def update_progress():
            """ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì¶œë ¥ (í•œ ì¤„ë¡œ)"""
            elapsed_time = asyncio.get_event_loop().time() - start_time
            rps = request_count / elapsed_time if elapsed_time > 0 else 0  # ì´ˆë‹¹ ìš”ì²­ ìˆ˜
            cps = total_candles_received / elapsed_time if elapsed_time > 0 else 0  # ì´ˆë‹¹ ìº”ë“¤ ìˆ˜
            
            processed_days = len(processed_dates)
            progress_pct = (processed_days / total_days) * 100 if total_days > 0 else 0
            
            latest_date_str = latest_processed_date.strftime("%Y-%m-%d") if latest_processed_date else "N/A"
            
            progress_msg = (f"\rğŸ“ˆ {symbol} {timeframe}: "
                          f"ìš”ì²­ {request_count:3d}/{max_requests} | "
                          f"RPS: {rps:4.1f} | "
                          f"CPS: {cps:5.0f} | "
                          f"ì§„í–‰: {processed_days:3d}/{total_days:3d}ì¼ ({progress_pct:5.1f}%) | "
                          f"ìµœì‹ : {latest_date_str}")
            
            print(progress_msg, end="", flush=True)
        
        try:
            # ê³¼ê±° ë°ì´í„° ìš”ì²­ì¸ì§€ í™•ì¸
            now_kst = datetime.now(ZoneInfo("Asia/Seoul"))
            is_historical_data = (now_kst - end_kst).days >= 1
            
            # ì´ˆê¸° ì§„í–‰ë¥  í‘œì‹œ
            update_progress()
            
            while current_end > start_kst and request_count < max_requests:
                params = params_base.copy()
                
                # ISO8601 + KST í¬ë§·ìœ¼ë¡œ 'to' íŒŒë¼ë¯¸í„° ì„¤ì •
                params["to"] = current_end.strftime("%Y-%m-%dT%H:%M:%S+09:00")
                
                params["count"] = self.max_candles_per_request
                
                request_count += 1
                
                # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                max_retries = 3
                retry_delay = 1
                candles = None
                
                for attempt in range(max_retries):
                    try:
                        async with session.get(url, params=params) as response:
                            response.raise_for_status()
                            candles = await response.json()
                            break  # ì„±ê³µì‹œ ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
                    except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                        if attempt < max_retries - 1:
                            await asyncio.sleep(retry_delay * (attempt + 1))  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
                        else:
                            raise e
                    
                if not candles:
                    break
                
                # KST ì‹œê°„ìœ¼ë¡œ íŒŒì‹±í•˜ê³  í•„í„°ë§ - ìš”ì²­ ë²”ìœ„ ë‚´ì˜ ë°ì´í„°ë§Œ ìˆ˜ì§‘
                valid_candles = []
                oldest_in_batch = None
                
                for candle in candles:
                    candle_time_str = candle["candle_date_time_kst"].replace("T", " ")
                    candle_time = datetime.fromisoformat(candle_time_str)
                    candle_time = candle_time.replace(tzinfo=ZoneInfo("Asia/Seoul"))
                    
                    # ìš”ì²­ ë²”ìœ„(start_kst <= candle_time <= end_kst) ë‚´ì˜ ë°ì´í„°ë§Œ ìˆ˜ì§‘
                    if start_kst <= candle_time <= end_kst:
                        valid_candles.append(candle)
                        # ì²˜ë¦¬ëœ ë‚ ì§œ ì¶”ê°€
                        processed_dates.add(candle_time.date())
                        if oldest_in_batch is None or candle_time < oldest_in_batch:
                            oldest_in_batch = candle_time
                    elif candle_time < start_kst:
                        # ì‹œì‘ ì‹œê°„ë³´ë‹¤ ì´ì „ ë°ì´í„° ë°œê²¬ì‹œ ë£¨í”„ ì¢…ë£Œ
                        break
                
                if valid_candles:
                    all_candles.extend(valid_candles)
                    total_candles_received += len(valid_candles)
                    
                    # ìµœì‹  ì²˜ë¦¬ ë‚ ì§œ ì—…ë°ì´íŠ¸
                    if oldest_in_batch:
                        latest_processed_date = oldest_in_batch
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    update_progress()
                else:
                    break
                
                # ë‹¤ìŒ ìš”ì²­ ì¤€ë¹„ - ì´ë²ˆ ë°°ì¹˜ì˜ ê°€ì¥ ì˜¤ë˜ëœ ì‹œê°„ ê¸°ì¤€
                if oldest_in_batch and oldest_in_batch > start_kst:
                    current_end = oldest_in_batch - timedelta(minutes=1)
                else:
                    break
                
                await asyncio.sleep(self.rate_limit_delay)
            
            # ìµœëŒ€ ìš”ì²­ ìˆ˜ ë„ë‹¬ì‹œ ì²˜ë¦¬
            if request_count >= max_requests:
                print(f"\nâš ï¸  ìµœëŒ€ ìš”ì²­ ìˆ˜ ({max_requests}) ë„ë‹¬")
            else:
                print()  # ë‹¤ìš´ë¡œë“œ ì™„ë£Œì‹œ ì¤„ë°”ê¿ˆ
                    
        finally:
            # ìµœì¢… í†µê³„ ì¶œë ¥
            elapsed_time = asyncio.get_event_loop().time() - start_time
            final_rps = request_count / elapsed_time if elapsed_time > 0 else 0
            final_cps = total_candles_received / elapsed_time if elapsed_time > 0 else 0
            processed_days = len(processed_dates)
            
            print(f"âœ… {symbol} {timeframe} ì™„ë£Œ: "
                  f"{total_candles_received}ê°œ ìº”ë“¤, {processed_days}/{total_days}ì¼, "
                  f"í‰ê·  {final_rps:.1f} RPS, {final_cps:.0f} CPS, "
                  f"{elapsed_time:.1f}ì´ˆ ì†Œìš”")
        
        # DataFrame ë³€í™˜
        if all_candles:
            df_data = []
            for candle in all_candles:
                timestamp_str = candle["candle_date_time_kst"].replace("T", " ")
                timestamp = datetime.fromisoformat(timestamp_str)
                timestamp = timestamp.replace(tzinfo=ZoneInfo("Asia/Seoul"))
                
                df_data.append({
                    "timestamp": timestamp,
                    "symbol": symbol,
                    "open": float(candle["opening_price"]),
                    "high": float(candle["high_price"]),
                    "low": float(candle["low_price"]),
                    "close": float(candle["trade_price"]),
                    "volume": float(candle["candle_acc_trade_volume"])
                })
            
            result_df = pl.DataFrame(df_data).sort("timestamp")
            return result_df
        
        # ë¹ˆ DataFrame ë°˜í™˜
        return pl.DataFrame(schema={
            "timestamp": pl.Datetime,
            "symbol": pl.Utf8,
            "open": pl.Float64,
            "high": pl.Float64,
            "low": pl.Float64,
            "close": pl.Float64,
            "volume": pl.Float64
        })
    
    async def _fetch_raw_data(
        self,
        symbols: List[str],
        start: datetime,
        end: datetime,
        timeframe: str
    ) -> pl.DataFrame:
        """ì›ì‹œ ë°ì´í„° ì¡°íšŒ - ìƒˆë¡œìš´ 3ë‹¨ê³„ ì‹œìŠ¤í…œ"""
        # ìš”ì²­ëœ íƒ€ì„í”„ë ˆì„ í™•ì¸
        if timeframe in self.SUPPORTED_API_TIMEFRAMES:
            api_timeframe = timeframe
        else:
            api_timeframe = "1m"  # ë¦¬ìƒ˜í”Œë§ìš©
        
        # UTC ë³€í™˜
        start_utc = self._to_utc(start)
        end_utc = self._to_utc(end)
        
        all_data = []
        
        for symbol in symbols:
            # 1ë‹¨ê³„: Parquet ìºì‹œ í™•ì¸
            cached_data = self.cache_manager.load_cache(
                self.PROVIDER_NAME, symbol, api_timeframe, start_utc, end_utc
            )
            
            if cached_data is not None:
                all_data.append(cached_data)
                continue
            
            # 2ë‹¨ê³„: SQLite ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ - ë¹ˆ ê¸°ê°„ ê³„ì‚° ë°©ì‹
            # ê¸°ì¡´ ë°ì´í„° ë²”ìœ„ ì¡°íšŒ
            existing_ranges = self._get_existing_data_ranges(
                symbol, api_timeframe, start_utc, end_utc
            )
            
            # ë¹ˆ ê¸°ê°„ ê³„ì‚°
            missing_periods = self._calculate_missing_periods(
                symbol, api_timeframe, start_utc, end_utc, existing_ranges
            )
            
            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¨¼ì € ì¶”ê°€
            if existing_ranges:
                db_data = self.db_manager.get_market_data(
                    self.PROVIDER_NAME, symbol, api_timeframe, start_utc, end_utc
                )
                if db_data.height > 0:
                    all_data.append(db_data)
            
            # ë¹ˆ ê¸°ê°„ì´ ìˆìœ¼ë©´ APIë¡œ ìš”ì²­
            if missing_periods:
                print(f"ğŸ“¥ {symbol} {api_timeframe}: {len(missing_periods)}ê°œ ë¹ˆ ê¸°ê°„ ë°œê²¬, API ìš”ì²­ ì‹œì‘...")
                
                # ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸
                if not await self._check_network_connection():
                    continue
                
                new_data_parts = []
                
                for i, period in enumerate(missing_periods):
                    period_start_kst = period["start"].astimezone(ZoneInfo("Asia/Seoul"))
                    period_end_kst = period["end"].astimezone(ZoneInfo("Asia/Seoul"))
                    
                    print(f"   ğŸ“¡ ë¹ˆ ê¸°ê°„ {i+1}/{len(missing_periods)}: "
                          f"{period_start_kst.strftime('%Y-%m-%d %H:%M')} ~ "
                          f"{period_end_kst.strftime('%Y-%m-%d %H:%M')} (KST)")
                    
                    # ë¹ˆ ê¸°ê°„ì— ëŒ€í•´ì„œë§Œ API ìš”ì²­
                    period_data = await self._fetch_candles_from_api(
                        symbol, api_timeframe, period_start_kst, period_end_kst
                    )
                    
                    if period_data.height > 0:
                        new_data_parts.append(period_data)
                        print(f"   âœ… {period_data.height}ê°œ ìº”ë“¤ ìˆ˜ì‹ ")
                    else:
                        print(f"   âš ï¸ ë°ì´í„° ì—†ìŒ")
                
                # ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ DBì— ì €ì¥
                if new_data_parts:
                    combined_new_data = pl.concat(new_data_parts)
                    
                    # SQLiteì— ì €ì¥
                    self.db_manager.save_market_data(
                        self.PROVIDER_NAME, symbol, api_timeframe, combined_new_data
                    )
                    
                    all_data.append(combined_new_data)
                    print(f"   ğŸ’¾ ì´ {combined_new_data.height}ê°œ ìƒˆ ìº”ë“¤ DB ì €ì¥ ì™„ë£Œ")
            
            # ìµœì¢… ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥ (ê¸°ì¡´ + ìƒˆ ë°ì´í„°)
            if all_data:
                # í˜„ì¬ ì‹¬ë³¼ì˜ ëª¨ë“  ë°ì´í„° ë‹¤ì‹œ ì¡°íšŒ (ê¸°ì¡´ + ìƒˆ ë°ì´í„°)
                final_data = self.db_manager.get_market_data(
                    self.PROVIDER_NAME, symbol, api_timeframe, start_utc, end_utc
                )
                
                if final_data.height > 0:
                    # Parquet ìºì‹œì— ì €ì¥
                    self.cache_manager.save_cache(
                        self.PROVIDER_NAME, symbol, api_timeframe, start_utc, end_utc, final_data
                    )
                    
                    # ê¸°ì¡´ì— ì¶”ê°€ëœ ë¶€ë¶„ ë°ì´í„°ë“¤ì„ ìµœì¢… ì™„ì „í•œ ë°ì´í„°ë¡œ êµì²´
                    # (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´)
                    all_data = [d for d in all_data if d.select("symbol").unique().item() != symbol]
                    all_data.append(final_data)
        
        # ëª¨ë“  ë°ì´í„° ê²°í•©
        if all_data:
            combined_data = pl.concat(all_data).sort(["symbol", "timestamp"])
            
            # ë¦¬ìƒ˜í”Œë§ í•„ìš”ì‹œ ì²˜ë¦¬
            if timeframe != api_timeframe:
                resampled_data = TimeframeUtils.resample_to_timeframe(
                    combined_data, timeframe, api_timeframe
                )
                return resampled_data
            
            return combined_data
        
        # ë¹ˆ DataFrame ë°˜í™˜
        return pl.DataFrame(schema={
            "timestamp": pl.Datetime,
            "symbol": pl.Utf8,
            "open": pl.Float64,
            "high": pl.Float64,
            "low": pl.Float64,
            "close": pl.Float64,
            "volume": pl.Float64
        })
    
    def get_storage_info(self) -> Dict[str, Any]:
        """ì €ì¥ì†Œ ì •ë³´ ë°˜í™˜"""
        info = {
            "provider": self.PROVIDER_NAME,
            "database": {
                "path": str(self.db_manager.db_path),
                "symbols_count": 0,
                "total_records": 0
            },
            "cache": self.cache_manager.get_cache_stats()
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¶”ê°€
        try:
            import sqlite3
            with sqlite3.connect(self.db_manager.db_path) as conn:
                # ì‹¬ë³¼ ìˆ˜
                cursor = conn.execute('''
                    SELECT COUNT(DISTINCT symbol) FROM market_data 
                    WHERE provider = ?
                ''', (self.PROVIDER_NAME,))
                info["database"]["symbols_count"] = cursor.fetchone()[0]
                
                # ì´ ë ˆì½”ë“œ ìˆ˜
                cursor = conn.execute('''
                    SELECT COUNT(*) FROM market_data 
                    WHERE provider = ?
                ''', (self.PROVIDER_NAME,))
                info["database"]["total_records"] = cursor.fetchone()[0]
        except:
            pass
        
        return info
    
    def cleanup_storage(self, days: int = 30):
        """ì €ì¥ì†Œ ì •ë¦¬"""
        # ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        db_deleted = self.db_manager.cleanup_old_data(days)
        cache_deleted = self.cache_manager.cleanup_old_cache(days // 4)  # ìºì‹œëŠ” ë” ìì£¼ ì •ë¦¬
    
    # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œë“¤ ì¶”ê°€
    def get_cache_info(self) -> Dict[str, Any]:
        """ìºì‹œ ì •ë³´ ë°˜í™˜ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        storage_info = self.get_storage_info()
        cache_info = storage_info["cache"]
        
        return {
            "cache_version": "2.0",
            "cache_dir": str(Path(self.cache_manager.cache_dir)),
            "cache_structure": "sqlite_parquet", 
            "cache_files_count": cache_info.get("total_files", 0),
            "cache_size_mb": cache_info.get("total_size_mb", 0.0),
            "symbols": cache_info.get("symbols", {}),
            "total_files": cache_info.get("total_files", 0),
            "total_size_mb": cache_info.get("total_size_mb", 0.0)
        }
    
    def clear_cache(self, symbol: Optional[str] = None, timeframe: Optional[str] = None):
        """ìºì‹œ ì‚­ì œ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        if symbol and timeframe:
            # íŠ¹ì • ì‹¬ë³¼ì˜ íŠ¹ì • íƒ€ì„í”„ë ˆì„ ìºì‹œ ì‚­ì œ
            self.cache_manager.clear_cache(symbol=symbol, timeframe=timeframe)
        elif symbol:
            # íŠ¹ì • ì‹¬ë³¼ì˜ ëª¨ë“  ìºì‹œ ì‚­ì œ
            self.cache_manager.clear_cache(symbol=symbol)
        else:
            # ëª¨ë“  ìºì‹œ ì‚­ì œ
            self.cache_manager.clear_cache()
    
    async def preload_data(
        self,
        symbols: List[str],
        start: datetime,
        end: datetime,
        timeframe: str,
        force_download: bool = False
    ) -> Dict[str, int]:
        """ë°ì´í„° ì‚¬ì „ ë¡œë“œ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        if force_download:
            for symbol in symbols:
                self.clear_cache(symbol=symbol, timeframe=timeframe)
        
        data = await self._fetch_raw_data(symbols, start, end, timeframe)
        
        result = {}
        for symbol in symbols:
            symbol_data = data.filter(pl.col("symbol") == symbol)
            result[symbol] = symbol_data.height
        
        return result
    
    def get_cached_data_info(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        """ìºì‹œëœ ë°ì´í„° ì •ë³´ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        return self.get_cache_info()
    
    # ìƒˆë¡œìš´ ìºì‹œ ì´ˆê¸°í™” ê¸°ëŠ¥ë“¤
    def clear_all_cache(self) -> Dict[str, Any]:
        """ëª¨ë“  ìºì‹œ ì™„ì „ ì´ˆê¸°í™”"""
        return self.cache_manager.clear_all_cache()
    
    def clear_old_cache(self, days: int = 7) -> Dict[str, Any]:
        """ì˜¤ë˜ëœ ìºì‹œ ì„ íƒ ì‚­ì œ"""
        return self.cache_manager.clear_old_cache(days)
    
    def clear_cache_by_criteria(self, provider: str = None, symbol: str = None, 
                               timeframe: str = None) -> Dict[str, Any]:
        """ì¡°ê±´ë³„ ìºì‹œ ì„ íƒ ì‚­ì œ (ê°œì„ ëœ ë²„ì „)"""
        if provider is None:
            provider = self.PROVIDER_NAME  # ê¸°ë³¸ê°’ìœ¼ë¡œ upbit ì‚¬ìš©
        return self.cache_manager.clear_cache_by_criteria(provider, symbol, timeframe)
    
    def find_cache_by_criteria(self, provider: str = None, symbol: str = None, 
                              timeframe: str = None) -> List[Dict[str, Any]]:
        """ì¡°ê±´ë³„ ìºì‹œ ê²€ìƒ‰"""
        if provider is None:
            provider = self.PROVIDER_NAME  # ê¸°ë³¸ê°’ìœ¼ë¡œ upbit ì‚¬ìš©
        return self.cache_manager.find_cache_by_criteria(provider, symbol, timeframe)
    
    def rebuild_cache_index(self) -> Dict[str, Any]:
        """ìºì‹œ ì¸ë±ìŠ¤ ì¬êµ¬ì„± (ê³ ì•„ íŒŒì¼ ì •ë¦¬)"""
        return self.cache_manager.rebuild_cache_index()
    
    def get_cache_health_report(self) -> Dict[str, Any]:
        """ìºì‹œ ìƒíƒœ ê±´ê°•ì„± ë³´ê³ ì„œ"""
        return self.cache_manager.get_cache_health_report()
    
    def auto_cleanup_cache_by_size(self, max_size_mb: float = 100.0) -> Dict[str, Any]:
        """ìºì‹œ í¬ê¸° ê¸°ë°˜ ìë™ ì •ë¦¬"""
        return self.cache_manager.auto_cleanup_by_size(max_size_mb)
    
    def cleanup_orphaned_cache(self) -> Dict[str, Any]:
        """ê³ ì•„ íŒŒì¼ ì •ë¦¬ ì „ìš© ë©”ì„œë“œ"""
        return self.cache_manager.check_and_cleanup_orphans()
    
    def print_cache_health_report(self):
        """ìºì‹œ ê±´ê°•ì„± ë³´ê³ ì„œë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥ (timeframe ì •ë³´ í¬í•¨)"""
        report = self.get_cache_health_report()
        
        print("=" * 60)
        print("ğŸ” ìºì‹œ ê±´ê°•ì„± ë³´ê³ ì„œ")
        print("=" * 60)
        
        if "error" in report:
            print(f"âŒ ì˜¤ë¥˜: {report['error']}")
            return
        
        print(f"\nğŸ“Š ìºì‹œ í†µê³„:")
        print(f"   â€¢ ìºì‹œ íŒŒì¼ ìˆ˜: {report['total_cache_files']:,}ê°œ")
        print(f"   â€¢ ë©”íƒ€ë°ì´í„° ì—”íŠ¸ë¦¬: {report['total_metadata_entries']:,}ê°œ")
        print(f"   â€¢ ì´ í¬ê¸°: {report['total_size_mb']:.1f} MB")
        print(f"   â€¢ ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ: {report['oldest_cache_days']}ì¼ ì „")
        
        # timeframeë³„ í†µê³„
        if report['timeframe_stats']:
            print(f"\nâ° íƒ€ì„í”„ë ˆì„ë³„ í†µê³„:")
            for timeframe, stats in report['timeframe_stats'].items():
                print(f"   â€¢ {timeframe}: {stats['cache_count']}ê°œ ìºì‹œ, "
                      f"{stats['total_records']:,}ê°œ ë ˆì½”ë“œ, "
                      f"í‰ê·  {stats['avg_records']:,}ê°œ/ìºì‹œ")
        
        # providerë³„ í†µê³„
        if report['provider_stats']:
            print(f"\nğŸ¢ í”„ë¡œë°”ì´ë”ë³„ í†µê³„:")
            for provider, count in report['provider_stats'].items():
                print(f"   â€¢ {provider}: {count}ê°œ ìºì‹œ")
        
        # symbolë³„ í†µê³„ (ìƒìœ„ 5ê°œë§Œ)
        if report['symbol_stats']:
            print(f"\nğŸ“ˆ ì‹¬ë³¼ë³„ í†µê³„ (ìƒìœ„ 5ê°œ):")
            for i, (symbol, count) in enumerate(list(report['symbol_stats'].items())[:5]):
                print(f"   â€¢ {symbol}: {count}ê°œ ìºì‹œ")
        
        print(f"\nğŸ”§ ìºì‹œ ìƒíƒœ:")
        print(f"   â€¢ ê³ ì•„ íŒŒì¼: {report['orphaned_files']}ê°œ")
        print(f"   â€¢ ëˆ„ë½ëœ íŒŒì¼: {report['missing_files']}ê°œ")
        print(f"   â€¢ ìºì‹œ íš¨ìœ¨ì„±: {report['cache_efficiency']:.1f}%")
        
        if report['recommendations']:
            print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for i, recommendation in enumerate(report['recommendations'], 1):
                print(f"   {i}. {recommendation}")
        else:
            print(f"\nâœ… ìºì‹œ ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆë‹¤!")
        
        print("\n" + "=" * 60)
    
    def print_cache_maintenance_menu(self):
        """ìºì‹œ ìœ ì§€ë³´ìˆ˜ ë©”ë‰´ ì¶œë ¥"""
        print("=" * 60)
        print("ğŸ› ï¸  CACHE MAINTENANCE MENU")
        print("=" * 60)
        
        # í˜„ì¬ ìƒíƒœ ìš”ì•½
        report = self.get_cache_health_report()
        print(f"ğŸ“Š í˜„ì¬ ìƒíƒœ:")
        print(f"   â€¢ ìºì‹œ íŒŒì¼: {report.get('total_cache_files', 0)}ê°œ")
        print(f"   â€¢ ì´ í¬ê¸°: {report.get('total_size_mb', 0):.1f}MB")
        print(f"   â€¢ ê³ ì•„ íŒŒì¼: {report.get('orphaned_files', 0)}ê°œ")
        print(f"   â€¢ ëˆ„ë½ ë©”íƒ€ë°ì´í„°: {report.get('missing_files', 0)}ê°œ")
        print(f"   â€¢ ìºì‹œ íš¨ìœ¨ì„±: {report.get('cache_efficiency', 0):.1f}%")
        
        print(f"\nğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:")
        print(f"   1. provider.cleanup_orphaned_cache()           # ê³ ì•„ íŒŒì¼ ì •ë¦¬")
        print(f"   2. provider.auto_cleanup_cache_by_size(100)    # í¬ê¸° ê¸°ë°˜ ì •ë¦¬ (100MB)")
        print(f"   3. provider.rebuild_cache_index()              # ì¸ë±ìŠ¤ ì¬êµ¬ì„±")
        print(f"   4. provider.clear_old_cache(7)                 # 7ì¼ ì´ìƒ ëœ ìºì‹œ ì‚­ì œ")
        print(f"   5. provider.clear_cache_by_criteria(symbol='KRW-BTC')  # ì¡°ê±´ë³„ ì‚­ì œ")
        
        # ìë™ ê¶Œì¥ì‚¬í•­
        if report.get('orphaned_files', 0) > 0 or report.get('missing_files', 0) > 0:
            print(f"\nğŸ’¡ ê¶Œì¥: cleanup_orphaned_cache() ì‹¤í–‰ ê¶Œì¥")
        
        if report.get('total_size_mb', 0) > 100:
            print(f"ğŸ’¡ ê¶Œì¥: auto_cleanup_cache_by_size(100) ì‹¤í–‰ ê¶Œì¥")
        
        print("\n" + "=" * 60)
    
    def get_cache_summary_by_timeframe(self) -> Dict[str, Any]:
        """íƒ€ì„í”„ë ˆì„ë³„ ìºì‹œ ìš”ì•½ ì •ë³´"""
        summary = {
            "timeframes": {},
            "total_caches": 0,
            "total_size_mb": 0.0
        }
        
        try:
            # ì „ì²´ ìºì‹œ ì°¾ê¸°
            all_caches = self.find_cache_by_criteria()
            summary["total_caches"] = len(all_caches)
            
            # íŒŒì¼ í¬ê¸° ê³„ì‚°
            cache_files = list(Path(self.cache_manager.cache_dir).glob("*.parquet"))
            total_size = sum(f.stat().st_size for f in cache_files if f.exists())
            summary["total_size_mb"] = total_size / (1024 * 1024)
            
            # íƒ€ì„í”„ë ˆì„ë³„ ê·¸ë£¹í™”
            timeframe_groups = {}
            for cache in all_caches:
                tf = cache.get('timeframe', 'unknown')
                if tf not in timeframe_groups:
                    timeframe_groups[tf] = []
                timeframe_groups[tf].append(cache)
            
            # ê° íƒ€ì„í”„ë ˆì„ë³„ í†µê³„ ê³„ì‚°
            for timeframe, caches in timeframe_groups.items():
                total_records = sum(cache.get('record_count', 0) for cache in caches)
                avg_records = total_records / len(caches) if caches else 0
                
                # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
                start_dates = [cache.get('start_time_utc') for cache in caches if cache.get('start_time_utc')]
                end_dates = [cache.get('end_time_utc') for cache in caches if cache.get('end_time_utc')]
                
                summary["timeframes"][timeframe] = {
                    "cache_count": len(caches),
                    "total_records": total_records,
                    "avg_records": int(avg_records),
                    "earliest_data": min(start_dates) if start_dates else None,
                    "latest_data": max(end_dates) if end_dates else None,
                    "symbols": list(set(cache.get('symbol') for cache in caches if cache.get('symbol')))
                }
        
        except Exception as e:
            summary["error"] = str(e)
        
        return summary
    
    def print_cache_summary_by_timeframe(self):
        """íƒ€ì„í”„ë ˆì„ë³„ ìºì‹œ ìš”ì•½ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        summary = self.get_cache_summary_by_timeframe()
        
        print("=" * 60)
        print("â° íƒ€ì„í”„ë ˆì„ë³„ ìºì‹œ ìš”ì•½")
        print("=" * 60)
        
        if "error" in summary:
            print(f"âŒ ì˜¤ë¥˜: {summary['error']}")
            return
        
        print(f"\nğŸ“Š ì „ì²´ ìš”ì•½:")
        print(f"   â€¢ ì´ ìºì‹œ ìˆ˜: {summary['total_caches']:,}ê°œ")
        print(f"   â€¢ ì´ í¬ê¸°: {summary['total_size_mb']:.1f} MB")
        
        if summary["timeframes"]:
            print(f"\nâ° íƒ€ì„í”„ë ˆì„ë³„ ìƒì„¸:")
            
            for timeframe, stats in summary["timeframes"].items():
                print(f"\nğŸ“… {timeframe} íƒ€ì„í”„ë ˆì„:")
                print(f"   â€¢ ìºì‹œ ìˆ˜: {stats['cache_count']:,}ê°œ")
                print(f"   â€¢ ì´ ë ˆì½”ë“œ: {stats['total_records']:,}ê°œ")
                print(f"   â€¢ í‰ê·  ë ˆì½”ë“œ/ìºì‹œ: {stats['avg_records']:,}ê°œ")
                
                if stats['earliest_data'] and stats['latest_data']:
                    print(f"   â€¢ ë°ì´í„° ë²”ìœ„: {stats['earliest_data'][:10]} ~ {stats['latest_data'][:10]}")
                
                if stats['symbols']:
                    symbol_count = len(stats['symbols'])
                    if symbol_count <= 5:
                        print(f"   â€¢ ì‹¬ë³¼: {', '.join(stats['symbols'])}")
                    else:
                        print(f"   â€¢ ì‹¬ë³¼: {', '.join(stats['symbols'][:3])} ì™¸ {symbol_count-3}ê°œ")
        
        print("\n" + "=" * 60)
    
    def cache_maintenance(self, auto_fix: bool = False) -> Dict[str, Any]:
        """ìºì‹œ ìë™ ìœ ì§€ë³´ìˆ˜"""
        result = {
            "health_report": None,
            "actions_taken": [],
            "success": True,
            "errors": []
        }
        
        try:
            # ê±´ê°•ì„± ë³´ê³ ì„œ ìƒì„±
            health_report = self.get_cache_health_report()
            result["health_report"] = health_report
            
            if "error" in health_report:
                result["errors"].append(f"ê±´ê°•ì„± ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {health_report['error']}")
                result["success"] = False
                return result
            
            actions_needed = []
            
            # ìë™ ìˆ˜ì •ì´ í•„ìš”í•œ ë¬¸ì œë“¤ ì‹ë³„
            if health_report['orphaned_files'] > 0 or health_report['missing_files'] > 0:
                actions_needed.append("index_rebuild")
            
            if health_report['oldest_cache_days'] > 30:
                actions_needed.append("old_cache_cleanup")
            
            if health_report['total_size_mb'] > 1000:  # 1GB ì´ìƒ
                actions_needed.append("size_optimization")
            
            if auto_fix and actions_needed:
                print("ğŸ”§ ìºì‹œ ìë™ ìœ ì§€ë³´ìˆ˜ ì‹œì‘...")
                
                # ì¸ë±ìŠ¤ ì¬êµ¬ì„±
                if "index_rebuild" in actions_needed:
                    rebuild_result = self.rebuild_cache_index()
                    if rebuild_result["success"]:
                        result["actions_taken"].append("ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì™„ë£Œ")
                    else:
                        result["errors"].extend(rebuild_result["errors"])
                
                # ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬
                if "old_cache_cleanup" in actions_needed:
                    cleanup_result = self.clear_old_cache(30)  # 30ì¼ ì´ìƒ ëœ ìºì‹œ ì •ë¦¬
                    if cleanup_result["success"]:
                        result["actions_taken"].append("ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                    else:
                        result["errors"].extend(cleanup_result["errors"])
                
                # í¬ê¸° ìµœì í™” (ì¶”ê°€ ì •ë¦¬)
                if "size_optimization" in actions_needed:
                    cleanup_result = self.clear_old_cache(14)  # 14ì¼ ì´ìƒ ëœ ìºì‹œ ì •ë¦¬
                    if cleanup_result["success"]:
                        result["actions_taken"].append("ìºì‹œ í¬ê¸° ìµœì í™” ì™„ë£Œ")
                    else:
                        result["errors"].extend(cleanup_result["errors"])
                
                print("âœ… ìºì‹œ ìë™ ìœ ì§€ë³´ìˆ˜ ì™„ë£Œ")
            
            elif actions_needed:
                result["actions_taken"].append(f"ê¶Œì¥ ì‘ì—…: {', '.join(actions_needed)}")
                print("ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ ìºì‹œ ìœ ì§€ë³´ìˆ˜ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”. auto_fix=Trueë¡œ ì„¤ì •í•˜ë©´ ìë™ ìˆ˜í–‰ë©ë‹ˆë‹¤.")
            
        except Exception as e:
            result["errors"].append(f"ìºì‹œ ìœ ì§€ë³´ìˆ˜ ì‹¤íŒ¨: {str(e)}")
            result["success"] = False
        
        return result
    
    # ìƒˆë¡œìš´ ë°ì´í„° í˜„í™© í™•ì¸ ê¸°ëŠ¥ë“¤
    def get_data_summary(self) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„° í˜„í™© ìš”ì•½"""
        summary = {
            "provider": self.PROVIDER_NAME,
            "database": {},
            "cache": {},
            "symbols": {}
        }
        
        try:
            import sqlite3
            with sqlite3.connect(self.db_manager.db_path) as conn:
                # ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ í†µê³„
                cursor = conn.execute('''
                    SELECT 
                        COUNT(DISTINCT symbol) as symbol_count,
                        COUNT(*) as total_records,
                        MIN(timestamp) as earliest_date,
                        MAX(timestamp) as latest_date
                    FROM market_data 
                    WHERE provider = ?
                ''', (self.PROVIDER_NAME,))
                
                row = cursor.fetchone()
                summary["database"] = {
                    "symbol_count": row[0],
                    "total_records": row[1],
                    "earliest_date": row[2],
                    "latest_date": row[3],
                    "path": str(self.db_manager.db_path)
                }
                
                # ì‹¬ë³¼ë³„ í†µê³„
                cursor = conn.execute('''
                    SELECT 
                        symbol,
                        timeframe,
                        COUNT(*) as record_count,
                        MIN(timestamp) as start_date,
                        MAX(timestamp) as end_date
                    FROM market_data 
                    WHERE provider = ?
                    GROUP BY symbol, timeframe
                    ORDER BY symbol, timeframe
                ''', (self.PROVIDER_NAME,))
                
                for row in cursor.fetchall():
                    symbol, timeframe, count, start_date, end_date = row
                    
                    if symbol not in summary["symbols"]:
                        summary["symbols"][symbol] = {}
                    
                    summary["symbols"][symbol][timeframe] = {
                        "record_count": count,
                        "start_date": start_date,
                        "end_date": end_date,
                        "data_source": "database"
                    }
        
        except Exception as e:
            summary["database"]["error"] = str(e)
        
        # ìºì‹œ ì •ë³´ ì¶”ê°€
        cache_info = self.cache_manager.get_cache_stats()
        summary["cache"] = cache_info
        
        return summary
    
    def get_symbol_data_range(self, symbol: str) -> Dict[str, Any]:
        """íŠ¹ì • ì‹¬ë³¼ì˜ ë°ì´í„° ë²”ìœ„ ìƒì„¸ ì •ë³´"""
        symbol_info = {
            "symbol": symbol,
            "database": {},
            "cache": {},
            "timeframes": {}
        }
        
        try:
            import sqlite3
            with sqlite3.connect(self.db_manager.db_path) as conn:
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•´ë‹¹ ì‹¬ë³¼ ì •ë³´ ì¡°íšŒ
                cursor = conn.execute('''
                    SELECT 
                        timeframe,
                        COUNT(*) as record_count,
                        MIN(timestamp) as start_date,
                        MAX(timestamp) as end_date,
                        MIN(close) as min_price,
                        MAX(close) as max_price,
                        AVG(volume) as avg_volume
                    FROM market_data 
                    WHERE provider = ? AND symbol = ?
                    GROUP BY timeframe
                    ORDER BY timeframe
                ''', (self.PROVIDER_NAME, symbol))
                
                for row in cursor.fetchall():
                    timeframe, count, start_date, end_date, min_price, max_price, avg_volume = row
                    
                    # ë°ì´í„° ì—°ì†ì„± í™•ì¸
                    start_dt = datetime.fromisoformat(start_date.replace("Z", "+00:00"))
                    end_dt = datetime.fromisoformat(end_date.replace("Z", "+00:00"))
                    
                    # ì˜ˆìƒ ìº”ë“¤ ìˆ˜ ê³„ì‚°
                    time_diff = end_dt - start_dt
                    if timeframe == "1m":
                        expected_count = int(time_diff.total_seconds() / 60)
                    elif timeframe == "1d":
                        expected_count = time_diff.days + 1
                    else:
                        expected_count = count
                    
                    coverage_rate = (count / expected_count * 100) if expected_count > 0 else 0
                    
                    symbol_info["timeframes"][timeframe] = {
                        "record_count": count,
                        "expected_count": expected_count,
                        "coverage_rate": round(coverage_rate, 2),
                        "start_date": start_date,
                        "end_date": end_date,
                        "price_range": {
                            "min": float(min_price) if min_price else None,
                            "max": float(max_price) if max_price else None
                        },
                        "avg_volume": float(avg_volume) if avg_volume else None,
                        "data_gaps": coverage_rate < 95  # 95% ë¯¸ë§Œì´ë©´ ë°ì´í„° ëˆ„ë½ ì˜ì‹¬
                    }
                
                # ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
                cursor = conn.execute('''
                    SELECT COUNT(*) FROM market_data 
                    WHERE provider = ? AND symbol = ?
                ''', (self.PROVIDER_NAME, symbol))
                
                symbol_info["database"]["total_records"] = cursor.fetchone()[0]
        
        except Exception as e:
            symbol_info["database"]["error"] = str(e)
        
        return symbol_info
    
    def list_available_data(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë°ì´í„° ëª©ë¡"""
        available_data = {
            "provider": self.PROVIDER_NAME,
            "symbols": [],
            "timeframes": set(),
            "date_ranges": {},
            "total_records": 0
        }
        
        try:
            import sqlite3
            with sqlite3.connect(self.db_manager.db_path) as conn:
                # ëª¨ë“  ì‹¬ë³¼ ëª©ë¡
                cursor = conn.execute('''
                    SELECT DISTINCT symbol FROM market_data 
                    WHERE provider = ?
                    ORDER BY symbol
                ''', (self.PROVIDER_NAME,))
                
                available_data["symbols"] = [row[0] for row in cursor.fetchall()]
                
                # ëª¨ë“  íƒ€ì„í”„ë ˆì„ ëª©ë¡
                cursor = conn.execute('''
                    SELECT DISTINCT timeframe FROM market_data 
                    WHERE provider = ?
                    ORDER BY timeframe
                ''', (self.PROVIDER_NAME,))
                
                available_data["timeframes"] = set(row[0] for row in cursor.fetchall())
                
                # ì „ì²´ ë‚ ì§œ ë²”ìœ„
                cursor = conn.execute('''
                    SELECT 
                        MIN(timestamp) as earliest,
                        MAX(timestamp) as latest,
                        COUNT(*) as total
                    FROM market_data 
                    WHERE provider = ?
                ''', (self.PROVIDER_NAME,))
                
                row = cursor.fetchone()
                if row[0]:  # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                    available_data["date_ranges"]["earliest"] = row[0]
                    available_data["date_ranges"]["latest"] = row[1]
                    available_data["total_records"] = row[2]
        
        except Exception as e:
            available_data["error"] = str(e)
        
        return available_data
    
    def check_data_coverage(
        self, 
        symbol: str, 
        timeframe: str, 
        start: datetime, 
        end: datetime
    ) -> Dict[str, Any]:
        """íŠ¹ì • ê¸°ê°„ì˜ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ í™•ì¸"""
        # íƒ€ì„ì¡´ ì •ê·œí™”
        start_utc = self._to_utc(start)
        end_utc = self._to_utc(end)
        
        coverage_info = {
            "symbol": symbol,
            "timeframe": timeframe,
            "requested_period": {
                "start": start_utc.isoformat(),
                "end": end_utc.isoformat()
            },
            "database": {
                "available": False,
                "record_count": 0,
                "coverage_rate": 0.0,
                "missing_periods": [],
                "existing_ranges": []
            },
            "cache": {
                "available": False,
                "cache_files": []
            }
        }
        
        try:
            # ê¸°ì¡´ ë°ì´í„° ë²”ìœ„ ì¡°íšŒ (ìŠ¤ë§ˆíŠ¸ ê°­ ê°ì§€ ë¡œì§ í™œìš©)
            existing_ranges = self._get_existing_data_ranges(
                symbol, timeframe, start_utc, end_utc
            )
            
            # ë¹ˆ ê¸°ê°„ ê³„ì‚° (ìŠ¤ë§ˆíŠ¸ ê°­ ê°ì§€ ë¡œì§ í™œìš©)
            missing_periods = self._calculate_missing_periods(
                symbol, timeframe, start_utc, end_utc, existing_ranges
            )
            
            # ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
            db_data = self.db_manager.get_market_data(
                self.PROVIDER_NAME, symbol, timeframe, start_utc, end_utc
            )
            
            if db_data.height > 0:
                coverage_info["database"]["available"] = True
                coverage_info["database"]["record_count"] = db_data.height
                
                # ì˜ˆìƒ ìº”ë“¤ ìˆ˜ ê³„ì‚°
                time_diff = end_utc - start_utc
                if timeframe == "1m":
                    expected_count = int(time_diff.total_seconds() / 60)
                elif timeframe == "1d":
                    expected_count = time_diff.days + 1
                else:
                    expected_count = db_data.height
                
                coverage_rate = (db_data.height / expected_count * 100) if expected_count > 0 else 0
                coverage_info["database"]["coverage_rate"] = round(coverage_rate, 2)
                coverage_info["database"]["expected_count"] = expected_count
                
                # ì‹¤ì œ ë°ì´í„° ë²”ìœ„
                timestamps = db_data.select("timestamp").to_series().to_list()
                if timestamps:
                    coverage_info["database"]["actual_start"] = min(timestamps).isoformat()
                    coverage_info["database"]["actual_end"] = max(timestamps).isoformat()
                
                # ê¸°ì¡´ ë°ì´í„° ë²”ìœ„ ì •ë³´ ì¶”ê°€
                coverage_info["database"]["existing_ranges"] = [
                    {
                        "start": range_info["start"].isoformat(),
                        "end": range_info["end"].isoformat()
                    }
                    for range_info in existing_ranges
                ]
                
                # ë¹ˆ ê¸°ê°„ ì •ë³´ ì¶”ê°€
                coverage_info["database"]["missing_periods"] = [
                    {
                        "start": period["start"].isoformat(),
                        "end": period["end"].isoformat()
                    }
                    for period in missing_periods
                ]
            else:
                # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì „ì²´ ê¸°ê°„ì´ ë¹ˆ ê¸°ê°„
                coverage_info["database"]["missing_periods"] = [{
                    "start": start_utc.isoformat(),
                    "end": end_utc.isoformat()
                }]
            
            # ìºì‹œ í™•ì¸
            cached_data = self.cache_manager.load_cache(
                self.PROVIDER_NAME, symbol, timeframe, start_utc, end_utc
            )
            
            if cached_data is not None:
                coverage_info["cache"]["available"] = True
                coverage_info["cache"]["record_count"] = cached_data.height
        
        except Exception as e:
            coverage_info["error"] = str(e)
        
        return coverage_info
    
    def print_data_summary(self):
        """ë°ì´í„° í˜„í™©ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        summary = self.get_data_summary()
        
        print("=" * 60)
        print(f"ğŸ“Š {summary['provider'].upper()} ë°ì´í„° í˜„í™© ìš”ì•½")
        print("=" * 60)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´
        db_info = summary.get("database", {})
        if "error" not in db_info:
            print(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ({db_info.get('path', 'N/A')})")
            print(f"   â€¢ ì‹¬ë³¼ ìˆ˜: {db_info.get('symbol_count', 0):,}ê°œ")
            print(f"   â€¢ ì´ ë ˆì½”ë“œ: {db_info.get('total_records', 0):,}ê°œ")
            print(f"   â€¢ ë°ì´í„° ê¸°ê°„: {db_info.get('earliest_date', 'N/A')} ~ {db_info.get('latest_date', 'N/A')}")
        
        # ìºì‹œ ì •ë³´
        cache_info = summary.get("cache", {})
        if cache_info:
            print(f"\nğŸ’¨ ìºì‹œ")
            print(f"   â€¢ ìºì‹œ íŒŒì¼: {cache_info.get('total_files', 0)}ê°œ")
            print(f"   â€¢ ìºì‹œ í¬ê¸°: {cache_info.get('total_size_mb', 0):.1f} MB")
        
        # ì‹¬ë³¼ë³„ ìƒì„¸ ì •ë³´
        symbols = summary.get("symbols", {})
        if symbols:
            print(f"\nğŸ“ˆ ì‹¬ë³¼ë³„ ë°ì´í„° ({len(symbols)}ê°œ)")
            print("-" * 60)
            
            for symbol, timeframes in symbols.items():
                print(f"\n{symbol}")
                for timeframe, info in timeframes.items():
                    count = info.get("record_count", 0)
                    start_date = info.get("start_date", "N/A")[:10] if info.get("start_date") else "N/A"
                    end_date = info.get("end_date", "N/A")[:10] if info.get("end_date") else "N/A"
                    print(f"   {timeframe:>4}: {count:>6,}ê°œ ({start_date} ~ {end_date})")
        
        print("\n" + "=" * 60) 